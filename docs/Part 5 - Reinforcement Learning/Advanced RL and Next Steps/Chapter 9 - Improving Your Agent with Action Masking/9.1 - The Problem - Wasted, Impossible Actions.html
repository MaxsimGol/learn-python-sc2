<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">9.1 - The Problem - Wasted, Impossible Actions | BurnySc2/Python-Sc2 Documentation</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://MaxsimGol.github.io/learn-python-sc2/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://MaxsimGol.github.io/learn-python-sc2/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="9.1 - The Problem - Wasted, Impossible Actions | BurnySc2/Python-Sc2 Documentation"><meta data-rh="true" name="description" content="The shift to a generalized, decomposed action space in our DRLRMBot architecture, while powerful, introduces a well-known and critical challenge in applied reinforcement learning: the Invalid Action Problem. The agent&#x27;s theoretical action space becomes vastly larger than its effective action space at any given moment, which severely impedes the learning process."><meta data-rh="true" property="og:description" content="The shift to a generalized, decomposed action space in our DRLRMBot architecture, while powerful, introduces a well-known and critical challenge in applied reinforcement learning: the Invalid Action Problem. The agent&#x27;s theoretical action space becomes vastly larger than its effective action space at any given moment, which severely impedes the learning process."><link data-rh="true" rel="icon" href="/learn-python-sc2/img/favicon.png"><link data-rh="true" rel="canonical" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions"><link data-rh="true" rel="alternate" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions" hreflang="en"><link data-rh="true" rel="alternate" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"9.1 - The Problem - Wasted, Impossible Actions","item":"https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YY6E18FRKL"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YY6E18FRKL",{anonymize_ip:!0})</script>



<meta name="google-site-verification" content="Ssfb9WLX16jSFbtw7JFWgG15TJY8nHTLIGt57oR9_uk"><link rel="stylesheet" href="/learn-python-sc2/assets/css/styles.254622d3.css">
<script src="/learn-python-sc2/assets/js/runtime~main.b6b0aae1.js" defer="defer"></script>
<script src="/learn-python-sc2/assets/js/main.8020269c.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/learn-python-sc2/"><b class="navbar__title text--truncate">Home page</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/learn-python-sc2/docs/Part 1 - Getting Started/Chapter 1 - Introduction/1.1 - What is python-sc2">Tutorial</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/MaxsimGol/learn-python-sc2" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 1 - Getting Started/Chapter 1 - Introduction/1.1 - What is python-sc2">Part 1 - Getting Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 2 - Core Concepts/Chapter 3 - The BotAI Class/3.1 - The Heart of Your Bot">Part 2 - Core Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 3 - Advanced Development/Chapter 6 - Building a Smarter Bot/6.1 - Macro-management - Expanding and Managing Your Economy">Part 3 - Advanced Development</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 4 - The Wider World/Chapter 8 - Bot Vision - Advantages and Limitations/8.1 - What Your Bot Sees That a Human Can&#x27;t - API Advantages">Part 4 - The Wider World</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2">Part 5 - Reinforcement Learning</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2">Foundations of Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 7 - Visualizing and Evaluating Training/7.1 - Setting up TensorBoard">Advanced RL and Next Steps</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 7 - Visualizing and Evaluating Training/7.1 - Setting up TensorBoard">Chapter 7 - Visualizing and Evaluating Training</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.1 - Concept - A More Generalized Agent">Chapter 8 - Advanced Implementation - DRL-RM</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions">Chapter 9 - Improving Your Agent with Action Masking</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions">9.1 - The Problem - Wasted, Impossible Actions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.2 - How Action Masking Works">9.2 - How Action Masking Works</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask">9.3 - Code - Implementing an Action Mask</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks">Chapter 10 - Improving Your Agent with Curriculum Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 11 - Conclusion and Further Exploration/11.1 - Summary of Your RL Journey">Chapter 11 - Conclusion and Further Exploration</a></div></li></ul></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/learn-python-sc2/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Part 5 - Reinforcement Learning</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Advanced RL and Next Steps</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Chapter 9 - Improving Your Agent with Action Masking</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">9.1 - The Problem - Wasted, Impossible Actions</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>9.1 - The Problem - Wasted, Impossible Actions</h1></header><p>The shift to a generalized, decomposed action space in our <code>DRL_RM_Bot</code> architecture, while powerful, introduces a well-known and critical challenge in applied reinforcement learning: the <strong>Invalid Action Problem</strong>. The agent&#x27;s theoretical action space becomes vastly larger than its <em>effective</em> action space at any given moment, which severely impedes the learning process.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="analysis--search-space-disparity"><strong>Analysis- Search Space Disparity</strong><a href="#analysis--search-space-disparity" class="hash-link" aria-label="Direct link to analysis--search-space-disparity" title="Direct link to analysis--search-space-disparity">​</a></h4>
<p>At any point in a game, the number of <em>valid</em> actions is a tiny subset of the <em>total possible</em> actions the agent can choose from.</p>
<p><strong>Diagram of the Action Space:</strong></p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|   Total Theoretical Action Space (10,000+ combinations)                  |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|       The RL agent&#x27;s random exploration is searching this entire area.   |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               +-----------------+                        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               | Effective (Valid)|                       |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               | Action Space    |                        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               | (e.g., ~10-100  |                        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               |  combinations)  |                        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                               +-----------------+                        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|                                                                          |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------------------------------------------------------+</span><br></span></code></pre></div></div>
<p>The agent is, in effect, searching a 100m-by-100m field for a coin.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="categorization-of-invalid-actions"><strong>Categorization of Invalid Actions</strong><a href="#categorization-of-invalid-actions" class="hash-link" aria-label="Direct link to categorization-of-invalid-actions" title="Direct link to categorization-of-invalid-actions">​</a></h4>
<p>Invalid actions in our <code>DRL_RM_Bot</code> can be broken down into two primary types:</p>
<table><thead><tr><th style="text-align:left">Category</th><th style="text-align:left">Definition</th><th style="text-align:left">Example</th></tr></thead><tbody><tr><td style="text-align:left"><strong>Out-of-Bounds (OOB) Invalidity</strong></td><td style="text-align:left">The action is invalid because a chosen index (<code>actor_id</code> or <code>target_id</code>) does not correspond to an existing unit in the current game state.</td><td style="text-align:left">The bot controls 5 Marines (valid <code>actor_id</code> values are <code>0-4</code>). The agent chooses <code>actor_id = 25</code>. This action is syntactically valid but semantically impossible.</td></tr><tr><td style="text-align:left"><strong>State-Based Invalidity</strong></td><td style="text-align:left">The action is syntactically valid (all indices are in-bounds), but the action is illegal according to the game&#x27;s rules engine.</td><td style="text-align:left">The agent chooses <code>actor_id = 3</code> (a Marine) and <code>target_id = 1</code> (another friendly Marine). The game engine forbids attacking friendly units, so this command would fail.</td></tr></tbody></table>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="impact-on-the-learning-process"><strong>Impact on the Learning Process</strong><a href="#impact-on-the-learning-process" class="hash-link" aria-label="Direct link to impact-on-the-learning-process" title="Direct link to impact-on-the-learning-process">​</a></h4>
<p>Allowing the agent to freely explore this vast space of invalid actions has severe, negative consequences for training.</p>
<ol>
<li>
<p><strong>Extreme Sample Inefficiency:</strong>
Reinforcement learning requires &quot;sampling&quot; the environment by trying actions. When over 99% of the actions an agent tries are invalid, the process of discovering a rare, valid action that leads to a positive reward becomes extraordinarily slow. The training budget is wasted on exploring meaningless choices.</p>
</li>
<li>
<p><strong>Gradient Signal Degradation:</strong>
The agent learns by updating its neural network based on the &quot;gradient&quot; of the reward signal. If the reward signal is almost always zero or a small penalty (for trying an invalid action), the gradient becomes sparse and noisy. The meaningful gradients from rare, successful actions are drowned out, making it difficult for the model to converge on an effective policy.</p>
</li>
</ol>
<p><strong>Conclusion:</strong>
For the learning problem to be tractable, we cannot permit the agent to explore the full theoretical action space. We must inject domain knowledge into the system to constrain its choices to only those that are valid at the current step. The standard architectural pattern for this is <strong>Action Masking</strong>.</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">8.3 - The Challenge of a Large Action Space</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.2 - How Action Masking Works"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">9.2 - How Action Masking Works</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Documentation Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/BurnySc2/python-sc2" target="_blank" rel="noopener noreferrer" class="footer__link-item">BurnySc2/python-sc2<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener noreferrer" class="footer__link-item">DLR-RM/stable-baselines3<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.com/invite/zXHU4wM" target="_blank" rel="noopener noreferrer" class="footer__link-item">SC2 AI Arena Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.youtube.com/watch?v=HlLK5BA0wT0&amp;list=PLQVvvaa0QuDcBby2qVDsDv41GghEQfr5E&amp;ab_channel=sentdex" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sentdex YouTube guide<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div></div></footer></div>
</body>
</html>