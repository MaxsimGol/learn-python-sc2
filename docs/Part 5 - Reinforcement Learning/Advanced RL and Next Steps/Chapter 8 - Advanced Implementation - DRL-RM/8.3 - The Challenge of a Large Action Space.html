<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.8.1">
<title data-rh="true">8.3 - The Challenge of a Large Action Space | Python-Sc2(BurnySc2) Tutorial: From Your First Role-Based Bot to Reinforcement Learning Agent</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://MaxsimGol.github.io/learn-python-sc2/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://MaxsimGol.github.io/learn-python-sc2/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="keywords" content="python-sc2, starcraft II, bot, ai, reinforcement learning, tutorial"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="8.3 - The Challenge of a Large Action Space | Python-Sc2(BurnySc2) Tutorial: From Your First Role-Based Bot to Reinforcement Learning Agent"><meta data-rh="true" name="description" content="The architectural shift to a generalized, entity-based agent provides immense flexibility but introduces a significant and well-known challenge in reinforcement learning: the combinatorial explosion of the action space."><meta data-rh="true" property="og:description" content="The architectural shift to a generalized, entity-based agent provides immense flexibility but introduces a significant and well-known challenge in reinforcement learning: the combinatorial explosion of the action space."><link data-rh="true" rel="icon" href="/learn-python-sc2/img/favicon.png"><link data-rh="true" rel="canonical" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space"><link data-rh="true" rel="alternate" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space" hreflang="en"><link data-rh="true" rel="alternate" href="https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space" hreflang="x-default"><script data-rh="true" type="application/ld+json">{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"8.3 - The Challenge of a Large Action Space","item":"https://MaxsimGol.github.io/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space"}]}</script><link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-YY6E18FRKL"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YY6E18FRKL",{anonymize_ip:!0})</script>



<meta name="google-site-verification" content="Ssfb9WLX16jSFbtw7JFWgG15TJY8nHTLIGt57oR9_uk"><link rel="stylesheet" href="/learn-python-sc2/assets/css/styles.254622d3.css">
<script src="/learn-python-sc2/assets/js/runtime~main.65c92ee8.js" defer="defer"></script>
<script src="/learn-python-sc2/assets/js/main.ccd29457.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<svg xmlns="http://www.w3.org/2000/svg" style="display: none;"><defs>
<symbol id="theme-svg-external-link" viewBox="0 0 24 24"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"/></symbol>
</defs></svg>
<script>!function(){var t="light";var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return window.localStorage.getItem("theme")}catch(t){}}();document.documentElement.setAttribute("data-theme",e||t),document.documentElement.setAttribute("data-theme-choice",e||t)}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="theme-layout-navbar navbar navbar--fixed-top"><div class="navbar__inner"><div class="theme-layout-navbar-left navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/learn-python-sc2/"><b class="navbar__title text--truncate">Home page</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/learn-python-sc2/docs/Part 1 - Getting Started/Chapter 1 - Introduction/1.1 - What is python-sc2">Tutorial</a></div><div class="theme-layout-navbar-right navbar__items navbar__items--right"><a href="https://github.com/MaxsimGol/learn-python-sc2" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">GitHub<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="system mode" aria-label="Switch between dark and light mode (currently system mode)"><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" aria-hidden="true" class="toggleIcon_g3eP systemToggleIcon_QzmC"><path fill="currentColor" d="m12 21c4.971 0 9-4.029 9-9s-4.029-9-9-9-9 4.029-9 9 4.029 9 9 9zm4.95-13.95c1.313 1.313 2.05 3.093 2.05 4.95s-0.738 3.637-2.05 4.95c-1.313 1.313-3.093 2.05-4.95 2.05v-14c1.857 0 3.637 0.737 4.95 2.05z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="theme-layout-main main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 1 - Getting Started/Chapter 1 - Introduction/1.1 - What is python-sc2">Part 1 - Getting Started</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 2 - Core Concepts/Chapter 3 - The BotAI Class/3.1 - The Heart of Your Bot">Part 2 - Core Concepts</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 3 - Advanced Development/Chapter 6 - Building a Smarter Bot/6.1 - Macro-management - Expanding and Managing Your Economy">Part 3 - Advanced Development</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" href="/learn-python-sc2/docs/Part 4 - The Wider World/Chapter 8 - Bot Vision - Advantages and Limitations/8.1 - What Your Bot Sees That a Human Can&#x27;t - API Advantages">Part 4 - The Wider World</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2">Part 5 - Reinforcement Learning</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2">Foundations of Reinforcement Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-2 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 7 - Visualizing and Evaluating Training/7.1 - Setting up TensorBoard">Advanced RL and Next Steps</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 7 - Visualizing and Evaluating Training/7.1 - Setting up TensorBoard">Chapter 7 - Visualizing and Evaluating Training</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" role="button" aria-expanded="true" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.1 - Concept - A More Generalized Agent">Chapter 8 - Advanced Implementation - DRL-RM</a></div><ul class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.1 - Concept - A More Generalized Agent">8.1 - Concept - A More Generalized Agent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation">8.2 - Code - A Simplified DRL-RM Implementation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-4 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space">8.3 - The Challenge of a Large Action Space</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions">Chapter 9 - Improving Your Agent with Action Masking</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks">Chapter 10 - Improving Your Agent with Curriculum Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-3 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" role="button" aria-expanded="false" tabindex="0" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 11 - Conclusion and Further Exploration/11.1 - Summary of Your RL Journey">Chapter 11 - Conclusion and Further Exploration</a></div></li></ul></li></ul></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/learn-python-sc2/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Part 5 - Reinforcement Learning</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Advanced RL and Next Steps</span></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Chapter 8 - Advanced Implementation - DRL-RM</span></li><li class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link">8.3 - The Challenge of a Large Action Space</span></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>8.3 - The Challenge of a Large Action Space</h1></header><p>The architectural shift to a generalized, entity-based agent provides immense flexibility but introduces a significant and well-known challenge in reinforcement learning: the <strong>combinatorial explosion of the action space</strong>.</p>
<p>Managing this complexity is the primary obstacle to successfully training a DRL-RM-style agent.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="problem--the-combinatorial-explosion"><strong>Problem- The Combinatorial Explosion</strong><a href="#problem--the-combinatorial-explosion" class="hash-link" aria-label="Direct link to problem--the-combinatorial-explosion" title="Direct link to problem--the-combinatorial-explosion">​</a></h4>
<p>A decomposed action space is combinatorial by nature. The total number of possible actions is the product of the sizes of its sub-actions.</p>
<p><strong>Action Space Size Comparison:</strong></p>
<table><thead><tr><th style="text-align:left">Agent</th><th style="text-align:left">Action Space Type</th><th style="text-align:left">Calculation</th><th style="text-align:left">Total Actions</th></tr></thead><tbody><tr><td style="text-align:left"><code>MicroBot</code></td><td style="text-align:left"><code>Discrete(3)</code></td><td style="text-align:left"><code>3</code></td><td style="text-align:left"><strong>3</strong></td></tr><tr><td style="text-align:left"><code>DRL_RM_Bot</code></td><td style="text-align:left"><code>MultiDiscrete()</code></td><td style="text-align:left"><code>50 (actors) * 2 (abilities) * 100 (targets)</code></td><td style="text-align:left"><strong>10,000</strong></td></tr></tbody></table>
<p>The agent&#x27;s decision space has expanded by over three orders of magnitude. This dramatically slows down the learning process, as the agent must explore a vastly larger set of possibilities.</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="the-invalid-action-problem"><strong>The Invalid Action Problem</strong><a href="#the-invalid-action-problem" class="hash-link" aria-label="Direct link to the-invalid-action-problem" title="Direct link to the-invalid-action-problem">​</a></h4>
<p>The core issue is that in any given game state, the vast majority of these 10,000 actions are <strong>invalid or nonsensical</strong>.</p>
<ul>
<li><strong>Invalid Indices:</strong> If the bot currently controls only 5 units, any action where <code>actor_index &gt; 4</code> is impossible.</li>
<li><strong>Invalid Targets:</strong> If there are only 10 total units on the map, any action where <code>target_index &gt; 9</code> is impossible.</li>
<li><strong>Logical Absurdities:</strong> The action &quot;Marine #2, Attack Marine #2&quot; is syntactically valid but strategically nonsensical.</li>
</ul>
<p>An agent using pure random exploration will spend the vast majority of its time selecting these invalid actions, receiving zero or negative rewards, and learning extremely slowly. The meaningful learning signal is drowned out by a sea of invalidity.</p>
<hr>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="solution--action-masking"><strong>Solution- Action Masking</strong><a href="#solution--action-masking" class="hash-link" aria-label="Direct link to solution--action-masking" title="Direct link to solution--action-masking">​</a></h4>
<p>The standard and most effective solution is <strong>Action Masking</strong>. This technique involves dynamically calculating which actions are valid at the current step and &quot;masking&quot; or forbidding the agent from choosing any invalid ones.</p>
<p><strong>The Action Masking Workflow:</strong>
Action masking modifies the interaction between the environment and the agent.</p>
<div class="language-text codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_QJqH"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  1. Environment State    |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| (e.g., we have 5 units)  |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  2. Generate Action Mask |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| (A boolean vector where  |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  True = a valid action)  |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">             v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+     +--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  3. Pass both Observation|     |  4. Agent&#x27;s Policy       |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|     AND Mask to Agent    |----&gt;|  (Neural Network)        |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+--------------------------+     +--------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                              |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                              | Applies mask to its raw output</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                              v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  5. Final Action Probability Distribution       |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">| (Probabilities for invalid actions are zeroed out) |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                              |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">                                              v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|  6. Agent samples an action that is GUARANTEED  |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">|     to be valid.                                |</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">+-------------------------------------------------+</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre></div></div>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Dramatically Prunes the Search Space:</strong> Focuses the agent&#x27;s exploration exclusively on valid and meaningful actions.</li>
<li><strong>Accelerates Learning:</strong> The agent receives a much cleaner and more consistent learning signal, leading to faster convergence on an effective policy.</li>
<li><strong>Enforces Game Rules:</strong> Action masking is a form of injecting domain knowledge into the agent, preventing it from having to learn basic game constraints from scratch.</li>
</ul>
<p>Implementing an action mask is an essential next step for training any agent with a large, decomposed action space. The next chapter will cover how to add this functionality to our environment.</p></div></article><nav class="docusaurus-mt-lg pagination-nav" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">8.2 - Code - A Simplified DRL-RM Implementation</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.1 - The Problem - Wasted, Impossible Actions"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">9.1 - The Problem - Wasted, Impossible Actions</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"></div></div></div></div></main></div></div></div><footer class="theme-layout-footer footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Documentation Resources</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://github.com/BurnySc2/python-sc2" target="_blank" rel="noopener noreferrer" class="footer__link-item">BurnySc2/python-sc2<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li><li class="footer__item"><a href="https://github.com/DLR-RM/stable-baselines3" target="_blank" rel="noopener noreferrer" class="footer__link-item">DLR-RM/stable-baselines3<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">Community</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://discord.com/invite/zXHU4wM" target="_blank" rel="noopener noreferrer" class="footer__link-item">SC2 AI Arena Discord<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div><div class="theme-layout-footer-column col footer__col"><div class="footer__title">More</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.youtube.com/watch?v=HlLK5BA0wT0&amp;list=PLQVvvaa0QuDcBby2qVDsDv41GghEQfr5E&amp;ab_channel=sentdex" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sentdex YouTube guide<svg width="13.5" height="13.5" aria-hidden="true" class="iconExternalLink_nPIU"><use href="#theme-svg-external-link"></use></svg></a></li></ul></div></div></div></footer></div>
</body>
</html>