This section provides the complete, updated code required to implement action masking in our `DRL_RM_Bot` environment. This is a critical upgrade that makes training the generalized agent feasible by focusing its exploration on valid actions only.

#### **Implementation Workflow**
*   [x] **Step 1:** Install the `sb3-contrib` library.
*   [x] **Step 2:** Update `drl_rm_bot.py` to support the new `Dict` observation space and generate the mask.
*   [x] **Step 3:** Update `train.py` to use the `MaskablePPO` algorithm.

---

#### **Step 1 - Install `sb3-contrib`**

In your activated virtual environment, install the companion library:
```sh
pip install sb3-contrib
```

---

#### **Step 2 - The Code: Updated `drl_rm_bot.py` with Masking Logic**

This is the modified version of our DRL-RM environment. The most significant changes are in the `DRL_RM_Env` class, which now defines a `Dict` space, and the `DRL_RM_Bot` class, which has a new method to generate the mask.

**`drl_rm_bot.py`**
```python
import numpy as np
import gymnasium as gym
from gymnasium.spaces import Box, Dict, MultiDiscrete
import queue

from sc2.bot_ai import BotAI
from sc2.race import Terran
from sc2.units import Units
from sc2_gym_env import SC2GymEnv

# --- Environment Constants ---
MAX_UNITS = 50
NUM_UNIT_FEATURES = 5
NUM_ABILITIES = 2 # 0=Move, 1=Attack

class DRL_RM_Bot(BotAI):
    """
    The DRL-RM bot, now updated to generate an action mask.
    """
    # NOTE: _encode_state() and _decode_and_execute_action() are kept from the previous section.
    # For brevity, only the new/changed methods are shown here.
    
    def __init__(self, action_queue, obs_queue):
        super().__init__()
        self.action_queue = action_queue
        self.obs_queue = obs_queue
        self.race = Terran

    def _get_action_mask(self) -> np.ndarray:
        """
        Calculates a flat boolean mask for all possible actions.
        An action is valid if its actor and target indices are within bounds.
        This is a more optimized version that avoids a large loop.
        """
        my_units_len = len(self.units)
        all_units_len = len(self.all_units)
        
        # A mask for valid actors (our units)
        valid_actors = np.arange(MAX_UNITS) < my_units_len
        # A mask for valid targets (all units)
        valid_targets = np.arange(MAX_UNITS * 2) < all_units_len
        
        # We can use numpy broadcasting to efficiently create the mask
        # The mask will be True only where actor, ability, and target are all valid.
        # This creates a (actors, abilities, targets) 3D mask
        mask_3d = np.logical_and(
            valid_actors[:, np.newaxis, np.newaxis],       # (50, 1, 1)
            valid_targets[np.newaxis, np.newaxis, :]        # (1, 1, 100)
        )
        
        # The ability dimension is always valid (size 2)
        final_mask = np.ones((MAX_UNITS, NUM_ABILITIES, MAX_UNITS * 2), dtype=bool)
        final_mask = final_mask & mask_3d
        
        # Flatten the 3D mask into a 1D vector for the agent
        return final_mask.flatten()

    async def on_step(self, iteration: int):
        # The main loop now follows a cleaner Observe -> Act cycle.
        if iteration % 8 == 0:
            # 1. OBSERVE: Get the current state and generate the observation and mask.
            observation = self._encode_state()
            action_mask = self._get_action_mask()
            
            # The observation is now a dictionary
            obs_dict = {
                "observation": observation,
                "action_mask": action_mask
            }
            
            terminated = self.townhalls.amount == 0
            
            # 2. SEND OBSERVATION: Send the data to the agent and wait for an action.
            self.obs_queue.put((obs_dict, 0.1, terminated, False, {}))
            
            if terminated:
                await self.client.leave()
                return

            # 3. ACT: Get the (now guaranteed valid) action from the agent and execute it.
            try:
                action = self.action_queue.get_nowait()
                await self._decode_and_execute_action(action)
            except queue.Empty:
                pass


class DRL_RM_Env(SC2GymEnv):
    """The Gymnasium Wrapper, updated to use a Dict observation space."""
    def __init__(self):
        super().__init__(bot_class=DRL_RM_Bot, map_name="AcropolisLE")
        
        self.action_space = MultiDiscrete([MAX_UNITS, NUM_ABILITIES, MAX_UNITS * 2])
        
        # The observation space must now be a Dict space.
        self.observation_space = Dict({
            # The key "observation" holds our original unit feature matrix
            "observation": Box(low=0, high=1, shape=(MAX_UNITS, NUM_UNIT_FEATURES), dtype=np.float32),
            # The key "action_mask" holds the boolean mask
            "action_mask": Box(low=0, high=1, shape=(self.action_space.nvec.prod(),), dtype=np.int8)
        })

```

---

#### **Step 3 - The Code: Updated `train.py`**

Your training script requires a single, simple change: importing and using `MaskablePPO`.

```python
# train_masked.py
import multiprocessing as mp
# Import from sb3_contrib instead of stable_baselines3
from sb3_contrib import MaskablePPO
from drl_rm_bot import DRL_RM_Env

def main():
    env = DRL_RM_Env()
    
    # Use the MaskablePPO class
    model = MaskablePPO("MlpPolicy", env, verbose=1)
    
    model.learn(total_timesteps=200_000)
    model.save("ppo_masked_drl_rm")
    env.close()

if __name__ == '__main__':
    mp.freeze_support()
    main()
```
With these modifications, your framework is now equipped with action masking, a crucial technique for making the complex DRL-RM agent trainable.