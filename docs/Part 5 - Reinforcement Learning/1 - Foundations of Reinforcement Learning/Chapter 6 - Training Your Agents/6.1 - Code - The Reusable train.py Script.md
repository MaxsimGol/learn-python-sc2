This `train.py` script serves as the main entry point for our entire reinforcement learning application. It is the top-level, synchronous process that orchestrates the creation of the learning environment, the training of the agent, and the saving of the final model.

The script is designed to be reusable; by changing a single configuration variable, you can switch between training the `WorkerEnv`, `MacroEnv`, or `MicroEnv`.

#### **The Training Workflow**

This script follows the standard machine learning training pipeline.

```
+--------------------------+
| 1. Select & Import Env   |
+--------------------------+
             |
             v
+--------------------------+
| 2. Instantiate Env       |
+--------------------------+
             |
             v
+--------------------------+
| 3. Instantiate RL Model  |
|    (e.g., PPO)           |
+--------------------------+
             |
             v
+--------------------------+
| 4. Run Training Loop     |
|    (model.learn)         |
+--------------------------+
             |
             v
+--------------------------+
| 5. Save Trained Model    |
+--------------------------+
```

---

#### **The Code: `train.py`**

```python
# train.py

import multiprocessing as mp

from stable_baselines3 import PPO

# --- Step 1: Import all possible environments ---
from worker_bot import WorkerEnv
from macro_bot import MacroEnv
from micro_bot import MicroEnv

# --- Configuration: Select the environment to train ---
# Use a dictionary to map names to environment classes for clean selection.
ENVIRONMENTS = {
    "worker": WorkerEnv,
    "macro": MacroEnv,
    "micro": MicroEnv,
}
# CHANGE THIS VALUE to select which environment to train
SELECTED_ENV = "micro"


def main():
    """The main function to configure and run the training process."""
    
    print(f"--- Starting training for environment: {SELECTED_ENV.upper()} ---")

    # --- Step 2: Instantiate the selected environment ---
    env_class = ENVIRONMENTS[SELECTED_ENV]
    env = env_class()
    
    # --- Step 3: Instantiate the PPO model ---
    # Proximal Policy Optimization (PPO) is a robust, general-purpose RL algorithm.
    # "MlpPolicy" specifies that the agent's brain will be a Multi-Layer Perceptron network.
    model = PPO(
        "MlpPolicy",
        env,
        verbose=1,
        tensorboard_log="./sc2_rl_tensorboard/"
    )

    # --- Step 4: Start the training loop ---
    # total_timesteps is the number of agent-environment interactions to perform.
    print(f"--- Beginning training for {100_000:,} timesteps... ---")
    model.learn(total_timesteps=100_000, progress_bar=True)
    print("--- Training complete. ---")

    # --- Step 5: Save the trained model ---
    # The learned policy is saved so it can be loaded later for evaluation.
    model_path = f"ppo_sc2_{SELECTED_ENV}.zip"
    model.save(model_path)
    print(f"--- Model saved to {model_path} ---")

    # --- Final Step: Clean up the environment ---
    # This ensures the StarCraft II game process is terminated.
    env.close()

if __name__ == '__main__':
    # The 'freeze_support()' line is a necessary guard for multiprocessing
    # on Windows and macOS. It ensures that the script can be safely
    # imported by child processes without causing recursive process creation.
    mp.freeze_support()
    main()
```