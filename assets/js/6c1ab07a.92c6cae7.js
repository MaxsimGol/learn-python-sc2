"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[3058],{6099:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask","title":"9.3 - Code - Implementing an Action Mask","description":"This section provides the complete, updated code required to implement action masking in our DRLRMBot environment. This is a critical upgrade that makes training the generalized agent feasible by focusing its exploration on valid actions only.","source":"@site/docs/Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/03-Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask.md","sourceDirName":"Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/03-Chapter 9 - Improving Your Agent with Action Masking","slug":"/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"9.2 - How Action Masking Works","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.2 - How Action Masking Works"},"next":{"title":"10.1 - The Concept - From Simple to Complex Tasks","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks"}}');var a=t(4848),i=t(8453);const s={},o=void 0,c={},l=[{value:"<strong>Implementation Workflow</strong>",id:"implementation-workflow",level:4},{value:"<strong>Step 1 - Install <code>sb3-contrib</code></strong>",id:"step-1---install-sb3-contrib",level:4},{value:"<strong>Step 2 - The Code: Updated <code>drl_rm_bot.py</code> with Masking Logic</strong>",id:"step-2---the-code-updated-drl_rm_botpy-with-masking-logic",level:4},{value:"<strong>Step 3 - The Code: Updated <code>train.py</code></strong>",id:"step-3---the-code-updated-trainpy",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsxs)(n.p,{children:["This section provides the complete, updated code required to implement action masking in our ",(0,a.jsx)(n.code,{children:"DRL_RM_Bot"})," environment. This is a critical upgrade that makes training the generalized agent feasible by focusing its exploration on valid actions only."]}),"\n",(0,a.jsx)(n.h4,{id:"implementation-workflow",children:(0,a.jsx)(n.strong,{children:"Implementation Workflow"})}),"\n",(0,a.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Step 1:"})," Install the ",(0,a.jsx)(n.code,{children:"sb3-contrib"})," library."]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Step 2:"})," Update ",(0,a.jsx)(n.code,{children:"drl_rm_bot.py"})," to support the new ",(0,a.jsx)(n.code,{children:"Dict"})," observation space and generate the mask."]}),"\n",(0,a.jsxs)(n.li,{className:"task-list-item",children:[(0,a.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,a.jsx)(n.strong,{children:"Step 3:"})," Update ",(0,a.jsx)(n.code,{children:"train.py"})," to use the ",(0,a.jsx)(n.code,{children:"MaskablePPO"})," algorithm."]}),"\n"]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h4,{id:"step-1---install-sb3-contrib",children:(0,a.jsxs)(n.strong,{children:["Step 1 - Install ",(0,a.jsx)(n.code,{children:"sb3-contrib"})]})}),"\n",(0,a.jsx)(n.p,{children:"In your activated virtual environment, install the companion library:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-sh",children:"pip install sb3-contrib\n"})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h4,{id:"step-2---the-code-updated-drl_rm_botpy-with-masking-logic",children:(0,a.jsxs)(n.strong,{children:["Step 2 - The Code: Updated ",(0,a.jsx)(n.code,{children:"drl_rm_bot.py"})," with Masking Logic"]})}),"\n",(0,a.jsxs)(n.p,{children:["This is the modified version of our DRL-RM environment. The most significant changes are in the ",(0,a.jsx)(n.code,{children:"DRL_RM_Env"})," class, which now defines a ",(0,a.jsx)(n.code,{children:"Dict"})," space, and the ",(0,a.jsx)(n.code,{children:"DRL_RM_Bot"})," class, which has a new method to generate the mask."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:(0,a.jsx)(n.code,{children:"drl_rm_bot.py"})})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport gymnasium as gym\r\nfrom gymnasium.spaces import Box, Dict, MultiDiscrete\r\nimport queue\r\n\r\nfrom sc2.bot_ai import BotAI\r\nfrom sc2.race import Terran\r\nfrom sc2.units import Units\r\nfrom sc2_gym_env import SC2GymEnv\r\n\r\n# --- Environment Constants ---\r\nMAX_UNITS = 50\r\nNUM_UNIT_FEATURES = 5\r\nNUM_ABILITIES = 2 # 0=Move, 1=Attack\r\n\r\nclass DRL_RM_Bot(BotAI):\r\n    """\r\n    The DRL-RM bot, now updated to generate an action mask.\r\n    """\r\n    # NOTE: _encode_state() and _decode_and_execute_action() are kept from the previous section.\r\n    # For brevity, only the new/changed methods are shown here.\r\n    \r\n    def __init__(self, action_queue, obs_queue):\r\n        super().__init__()\r\n        self.action_queue = action_queue\r\n        self.obs_queue = obs_queue\r\n        self.race = Terran\r\n\r\n    def _get_action_mask(self) -> np.ndarray:\r\n        """\r\n        Calculates a flat boolean mask for all possible actions.\r\n        An action is valid if its actor and target indices are within bounds.\r\n        This is a more optimized version that avoids a large loop.\r\n        """\r\n        my_units_len = len(self.units)\r\n        all_units_len = len(self.all_units)\r\n        \r\n        # A mask for valid actors (our units)\r\n        valid_actors = np.arange(MAX_UNITS) < my_units_len\r\n        # A mask for valid targets (all units)\r\n        valid_targets = np.arange(MAX_UNITS * 2) < all_units_len\r\n        \r\n        # We can use numpy broadcasting to efficiently create the mask\r\n        # The mask will be True only where actor, ability, and target are all valid.\r\n        # This creates a (actors, abilities, targets) 3D mask\r\n        mask_3d = np.logical_and(\r\n            valid_actors[:, np.newaxis, np.newaxis],       # (50, 1, 1)\r\n            valid_targets[np.newaxis, np.newaxis, :]        # (1, 1, 100)\r\n        )\r\n        \r\n        # The ability dimension is always valid (size 2)\r\n        final_mask = np.ones((MAX_UNITS, NUM_ABILITIES, MAX_UNITS * 2), dtype=bool)\r\n        final_mask = final_mask & mask_3d\r\n        \r\n        # Flatten the 3D mask into a 1D vector for the agent\r\n        return final_mask.flatten()\r\n\r\n    async def on_step(self, iteration: int):\r\n        # The main loop now follows a cleaner Observe -> Act cycle.\r\n        if iteration % 8 == 0:\r\n            # 1. OBSERVE: Get the current state and generate the observation and mask.\r\n            observation = self._encode_state()\r\n            action_mask = self._get_action_mask()\r\n            \r\n            # The observation is now a dictionary\r\n            obs_dict = {\r\n                "observation": observation,\r\n                "action_mask": action_mask\r\n            }\r\n            \r\n            terminated = self.townhalls.amount == 0\r\n            \r\n            # 2. SEND OBSERVATION: Send the data to the agent and wait for an action.\r\n            self.obs_queue.put((obs_dict, 0.1, terminated, False, {}))\r\n            \r\n            if terminated:\r\n                await self.client.leave()\r\n                return\r\n\r\n            # 3. ACT: Get the (now guaranteed valid) action from the agent and execute it.\r\n            try:\r\n                action = self.action_queue.get_nowait()\r\n                await self._decode_and_execute_action(action)\r\n            except queue.Empty:\r\n                pass\r\n\r\n\r\nclass DRL_RM_Env(SC2GymEnv):\r\n    """The Gymnasium Wrapper, updated to use a Dict observation space."""\r\n    def __init__(self):\r\n        super().__init__(bot_class=DRL_RM_Bot, map_name="AcropolisLE")\r\n        \r\n        self.action_space = MultiDiscrete([MAX_UNITS, NUM_ABILITIES, MAX_UNITS * 2])\r\n        \r\n        # The observation space must now be a Dict space.\r\n        self.observation_space = Dict({\r\n            # The key "observation" holds our original unit feature matrix\r\n            "observation": Box(low=0, high=1, shape=(MAX_UNITS, NUM_UNIT_FEATURES), dtype=np.float32),\r\n            # The key "action_mask" holds the boolean mask\r\n            "action_mask": Box(low=0, high=1, shape=(self.action_space.nvec.prod(),), dtype=np.int8)\r\n        })\r\n\n'})}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h4,{id:"step-3---the-code-updated-trainpy",children:(0,a.jsxs)(n.strong,{children:["Step 3 - The Code: Updated ",(0,a.jsx)(n.code,{children:"train.py"})]})}),"\n",(0,a.jsxs)(n.p,{children:["Your training script requires a single, simple change: importing and using ",(0,a.jsx)(n.code,{children:"MaskablePPO"}),"."]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# train_masked.py\r\nimport multiprocessing as mp\r\n# Import from sb3_contrib instead of stable_baselines3\r\nfrom sb3_contrib import MaskablePPO\r\nfrom drl_rm_bot import DRL_RM_Env\r\n\r\ndef main():\r\n    env = DRL_RM_Env()\r\n    \r\n    # Use the MaskablePPO class\r\n    model = MaskablePPO("MlpPolicy", env, verbose=1)\r\n    \r\n    model.learn(total_timesteps=200_000)\r\n    model.save("ppo_masked_drl_rm")\r\n    env.close()\r\n\r\nif __name__ == \'__main__\':\r\n    mp.freeze_support()\r\n    main()\n'})}),"\n",(0,a.jsx)(n.p,{children:"With these modifications, your framework is now equipped with action masking, a crucial technique for making the complex DRL-RM agent trainable."})]})}function p(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const a={},i=r.createContext(a);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);