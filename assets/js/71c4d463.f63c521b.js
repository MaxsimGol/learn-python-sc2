"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[1687],{8453:(n,e,r)=>{r.d(e,{R:()=>a,x:()=>s});var t=r(6540);const o={},i=t.createContext(o);function a(n){const e=t.useContext(i);return t.useMemo((function(){return"function"==typeof n?n(e):{...e,...n}}),[e,n])}function s(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(i.Provider,{value:e},n.children)}},9592:(n,e,r)=>{r.r(e),r.d(e,{assets:()=>c,contentTitle:()=>s,default:()=>h,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.1 - Code - The Reusable train.py Script","title":"6.1 - Code - The Reusable train.py Script","description":"This train.py script serves as the main entry point for our entire reinforcement learning application. It is the top-level, synchronous process that orchestrates the creation of the learning environment, the training of the agent, and the saving of the final model.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.1 - Code - The Reusable train.py Script.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.1 - Code - The Reusable train.py Script","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.1 - Code - The Reusable train.py Script","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"5.3 - Code - The Micro Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.3 - Code - The Micro Bot Environment"},"next":{"title":"6.2 - A Quick Look at the PPO Algorithm","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.2 - A Quick Look at the PPO Algorithm"}}');var o=r(4848),i=r(8453);const a={},s=void 0,c={},l=[{value:"<strong>The Training Workflow</strong>",id:"the-training-workflow",level:4},{value:"<strong>The Code: <code>train.py</code></strong>",id:"the-code-trainpy",level:4}];function p(n){const e={code:"code",h4:"h4",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,i.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(e.p,{children:["This ",(0,o.jsx)(e.code,{children:"train.py"})," script serves as the main entry point for our entire reinforcement learning application. It is the top-level, synchronous process that orchestrates the creation of the learning environment, the training of the agent, and the saving of the final model."]}),"\n",(0,o.jsxs)(e.p,{children:["The script is designed to be reusable; by changing a single configuration variable, you can switch between training the ",(0,o.jsx)(e.code,{children:"WorkerEnv"}),", ",(0,o.jsx)(e.code,{children:"MacroEnv"}),", or ",(0,o.jsx)(e.code,{children:"MicroEnv"}),"."]}),"\n",(0,o.jsx)(e.h4,{id:"the-training-workflow",children:(0,o.jsx)(e.strong,{children:"The Training Workflow"})}),"\n",(0,o.jsx)(e.p,{children:"This script follows the standard machine learning training pipeline."}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"+--------------------------+\r\n| 1. Select & Import Env   |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 2. Instantiate Env       |\r\n|    (Vectorized)          |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 3. Instantiate RL Model  |\r\n|    (e.g., PPO)           |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 4. Run Training Loop     |\r\n|    (model.learn)         |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 5. Save Trained Model    |\r\n+--------------------------+\n"})}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h4,{id:"the-code-trainpy",children:(0,o.jsxs)(e.strong,{children:["The Code: ",(0,o.jsx)(e.code,{children:"train.py"})]})}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# train.py\r\n\r\nfrom stable_baselines3 import PPO\r\nfrom stable_baselines3.common.env_util import make_vec_env\r\n\r\n# --- Step 1: Import all possible environments ---\r\n# These custom environment classes must follow the Gymnasium API specification.\r\nfrom worker_bot import WorkerEnv\r\nfrom macro_bot import MacroEnv\r\nfrom micro_bot import MicroEnv\r\n\r\n# --- Configuration: Select the environment to train ---\r\n# Use a dictionary to map names to environment classes for clean selection.\r\nENVIRONMENTS = {\r\n    "worker": WorkerEnv,\r\n    "macro": MacroEnv,\r\n    "micro": MicroEnv,\r\n}\r\n# CHANGE THIS VALUE to select which environment to train\r\nSELECTED_ENV = "micro"\r\n\r\n\r\ndef main():\r\n    """The main function to configure and run the training process."""\r\n    \r\n    print(f"--- Starting training for environment: {SELECTED_ENV.upper()} ---")\r\n\r\n    # --- Step 2: Instantiate the selected environment ---\r\n    # We use make_vec_env to create a vectorized environment.\r\n    # This is the standard in Stable Baselines3 for robust environment handling.\r\n    # It runs the environment in a separate process, which is required for StarCraft II.\r\n    env_class = ENVIRONMENTS[SELECTED_ENV]\r\n    env = make_vec_env(env_class, n_envs=1)\r\n    \r\n    # --- Step 3: Instantiate the PPO model ---\r\n    # Proximal Policy Optimization (PPO) is a robust, general-purpose RL algorithm.\r\n    # "MlpPolicy" specifies that the agent\'s brain will be a Multi-Layer Perceptron network.\r\n    model = PPO(\r\n        "MlpPolicy",\r\n        env,\r\n        verbose=1,\r\n        tensorboard_log="./sc2_rl_tensorboard/"\r\n    )\r\n\r\n    # --- Step 4: Start the training loop ---\r\n    # total_timesteps is the number of agent-environment interactions to perform.\r\n    print(f"--- Beginning training for {100_000:,} timesteps... ---")\r\n    model.learn(total_timesteps=100_000, progress_bar=True)\r\n    print("--- Training complete. ---")\r\n\r\n    # --- Step 5: Save the trained model ---\r\n    # The learned policy is saved so it can be loaded later for evaluation.\r\n    model_path = f"ppo_sc2_{SELECTED_ENV}.zip"\r\n    model.save(model_path)\r\n    print(f"--- Model saved to {model_path} ---")\r\n\r\n    # --- Final Step: Clean up the environment ---\r\n    # This ensures the vectorized environment and the StarCraft II game process are terminated.\r\n    env.close()\r\n\r\nif __name__ == \'__main__\':\r\n    # This guard is necessary for multiprocessing, which `make_vec_env` uses.\r\n    # It prevents child processes from re-importing and re-executing the main script,\r\n    # which would lead to an infinite loop of process creation on some platforms.\r\n    main()\n'})})]})}function h(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}}}]);