"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[2676],{6018:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>h,frontMatter:()=>a,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.1 - Goal - Learning a Combat Skill - Kiting","title":"5.1 - Goal - Learning a Combat Skill - Kiting","description":"This chapter marks a transition from macroeconomic tasks to micro-management. Our third agent will be trained to master a fundamental combat tactic known as kiting or \\"stutter-stepping.\\"","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.1 - Goal - Learning a Combat Skill - Kiting.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.1 - Goal - Learning a Combat Skill - Kiting","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.1 - Goal - Learning a Combat Skill - Kiting","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"4.3 - Code - The Macro Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.3 - Code - The Macro Bot Environment"},"next":{"title":"5.2 - Design - Observations for Spatial Awareness","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.2 - Design - Observations for Spatial Awareness"}}');var s=t(4848),r=t(8453);const a={},l=void 0,o={},c=[{value:"<strong>The Desired Behavior- A State-Based Kiting Policy</strong>",id:"the-desired-behavior--a-state-based-kiting-policy",level:4},{value:"<strong>Success Criteria</strong>",id:"success-criteria",level:4},{value:"<strong>Controlled Scenario Setup</strong>",id:"controlled-scenario-setup",level:4},{value:"<strong>Environment Design</strong>",id:"environment-design",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(n.p,{children:["This chapter marks a transition from macroeconomic tasks to ",(0,s.jsx)(n.strong,{children:"micro-management"}),". Our third agent will be trained to master a fundamental combat tactic known as ",(0,s.jsx)(n.strong,{children:"kiting"}),' or "stutter-stepping."']}),"\n",(0,s.jsx)(n.p,{children:"The goal is to move beyond resource management and teach an agent to make optimal, split-second decisions in a dynamic, adversarial combat scenario."}),"\n",(0,s.jsx)(n.h4,{id:"the-desired-behavior--a-state-based-kiting-policy",children:(0,s.jsx)(n.strong,{children:"The Desired Behavior- A State-Based Kiting Policy"})}),"\n",(0,s.jsx)(n.p,{children:'Kiting is a policy for a ranged unit to defeat a faster melee attacker. The agent must learn to switch between "Attack" and "Move" states based on its weapon cooldown and its distance to the target.'}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"The Ideal Policy (Finite State Machine):"})}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"+------------------+     Target is in range     +-------------------+\r\n|                  | -------------------------\x3e |                   |\r\n|  State:          |                            |  State:           |\r\n|  MOVING TOWARDS  | <------------------------- |  ATTACKING        |\r\n|  TARGET          |    Target is out of range  |                   |\r\n|                  |                            |                   |\r\n+--------+---------+                            +---------+---------+\r\n         |                                                 |\r\n         | Target is in range                              | Weapon is on cooldown\r\n         |                                                 |\r\n         +---------------------v---------------------------+\r\n                               |\r\n                   +-----------v-----------+\r\n                   |                       |\r\n                   | State:                |\r\n                   | KITING (MOVING AWAY)  |\r\n                   |                       |\r\n                   +-----------^-----------+\r\n                               |\r\n                               | Weapon cooldown is over\r\n                               |\r\n                               +\n"})}),"\n",(0,s.jsx)(n.h4,{id:"success-criteria",children:(0,s.jsx)(n.strong,{children:"Success Criteria"})}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","The agent (a single Marine) must learn to consistently defeat a single, faster Zergling."]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","The agent's policy must correctly utilize ",(0,s.jsx)(n.code,{children:"weapon_cooldown"})," to decide when to move."]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","The agent's policy must correctly utilize ",(0,s.jsx)(n.code,{children:"target_in_range"})," to decide when to attack or reposition."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"controlled-scenario-setup",children:(0,s.jsx)(n.strong,{children:"Controlled Scenario Setup"})}),"\n",(0,s.jsx)(n.p,{children:"To isolate the kiting task, we will use debug commands to create a perfect, repeatable 1v1 scenario at the start of each episode."}),"\n",(0,s.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Spawn one friendly Marine for the agent to control."]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Spawn one enemy Zergling at a fixed distance."]}),"\n",(0,s.jsxs)(n.li,{className:"task-list-item",children:[(0,s.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Immediately remove all starting worker units."]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h4,{id:"environment-design",children:(0,s.jsx)(n.strong,{children:"Environment Design"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"1. Observation Space Specification"})}),"\n",(0,s.jsx)(n.p,{children:"The observation vector is designed to provide all critical information for a 1v1 combat engagement."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gymnasium Type:"})," ",(0,s.jsx)(n.code,{children:"gymnasium.spaces.Box"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Shape:"})," ",(0,s.jsx)(n.code,{children:"(5,)"})]}),"\n"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Index"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Feature"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Rationale"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"0"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"marine.health_percentage"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:'"How much danger am I in?"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"1"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"marine.weapon_cooldown > 0"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"(Key Signal)"}),' "Is my weapon ready to fire?"']})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"2"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"zergling.health_percentage"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:'"How close am I to winning?"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"3"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"marine.distance_to(zergling)"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:'"Is my positioning optimal?"'})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"4"})}),(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"marine.target_in_range"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"(Key Signal)"}),' "Is attacking a valid move from this position?"']})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"2. Action Space Specification"})}),"\n",(0,s.jsx)(n.p,{children:"The agent is given a set of discrete combat maneuvers."}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Gymnasium Type:"})," ",(0,s.jsx)(n.code,{children:"gymnasium.spaces.Discrete"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Size:"})," ",(0,s.jsx)(n.code,{children:"3"})]}),"\n"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Action"}),(0,s.jsx)(n.th,{style:{textAlign:"left"},children:"Agent's Intent"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"0"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"Attack Target:"})," Engage the enemy."]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"1"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"Move Away:"}),' Increase distance (the "kite" action).']})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{style:{textAlign:"left"},children:(0,s.jsx)(n.code,{children:"2"})}),(0,s.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,s.jsx)(n.strong,{children:"Move Towards:"}),' Decrease distance (the "chase" action).']})]})]})]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"3. Reward Function Specification"})}),"\n",(0,s.jsx)(n.p,{children:"The reward must directly incentivize combat efficiency."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Design Philosophy:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Health Differential:"})," The primary learning signal is the change in relative health between the two units. This directly rewards dealing damage while avoiding taking damage."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Large Terminal Reward:"})," A definitive win/loss signal at the end of the episode provides a clear, final objective."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Reward Calculation (on each step):"}),"\r\n",(0,s.jsx)(n.code,{children:"reward = (previous_zergling_hp - current_zergling_hp) - (previous_marine_hp - current_marine_hp)"})]}),"\n",(0,s.jsxs)(n.p,{children:["This creates a zero-sum reward where positive values mean the agent won the trade, and negative values mean it lost the trade. A large terminal reward (",(0,s.jsx)(n.code,{children:"+100"})," for a win, ",(0,s.jsx)(n.code,{children:"-100"})," for a loss) is added at the end."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var i=t(6540);const s={},r=i.createContext(s);function a(e){const n=i.useContext(r);return i.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);