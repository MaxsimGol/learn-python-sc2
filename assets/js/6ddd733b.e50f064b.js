"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[5740],{6356:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>c,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>a});const i=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.1 - Goal - Learning a Single Economic Task","title":"3.1 - Goal - Learning a Single Economic Task","description":"Our first foray into reinforcement learning will be a foundational \\"Hello, World!\\" task. The objective is to create the simplest possible environment where an agent can learn a single, vital economic behavior: producing worker units.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.1 - Goal - Learning a Single Economic Task.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.1 - Goal - Learning a Single Economic Task","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.1 - Goal - Learning a Single Economic Task","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"2.2 - Code - The Reusable SC2GymEnv Wrapper","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 2 - Setting Up the RL Environment/2.2 - Code - The Reusable SC2GymEnv Wrapper"},"next":{"title":"3.2 - Design - Observation, Action, and Reward","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward"}}');var r=n(4848),s=n(8453);const l={},o=void 0,c={},a=[{value:"<strong>Problem Definition</strong>",id:"problem-definition",level:4},{value:"<strong>Environment Design</strong>",id:"environment-design",level:4},{value:"<strong>Key Learning Challenges for the Agent</strong>",id:"key-learning-challenges-for-the-agent",level:4}];function d(e){const t={code:"code",h4:"h4",hr:"hr",input:"input",li:"li",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(t.p,{children:'Our first foray into reinforcement learning will be a foundational "Hello, World!" task. The objective is to create the simplest possible environment where an agent can learn a single, vital economic behavior: producing worker units.'}),"\n",(0,r.jsxs)(t.p,{children:["By isolating this one task, we can verify that our entire RL architecture\u2014from the ",(0,r.jsx)(t.code,{children:"SC2GymEnv"})," wrapper to the training script\u2014is functioning correctly before adding further complexity."]}),"\n",(0,r.jsx)(t.h4,{id:"problem-definition",children:(0,r.jsx)(t.strong,{children:"Problem Definition"})}),"\n",(0,r.jsx)(t.p,{children:"We will train an agent whose sole purpose is to learn the correct policy for building SCVs from a Command Center."}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Success Criteria:"}),"\n",(0,r.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(t.li,{className:"task-list-item",children:[(0,r.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","The agent must learn to check for sufficient resources (",(0,r.jsx)(t.code,{children:"minerals >= 50"}),")."]}),"\n",(0,r.jsxs)(t.li,{className:"task-list-item",children:[(0,r.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","The agent must learn to check for an available production facility (",(0,r.jsx)(t.code,{children:"idle Command Center"}),")."]}),"\n",(0,r.jsxs)(t.li,{className:"task-list-item",children:[(0,r.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ",'The agent must learn to correlate the correct game state with the "build worker" action.']}),"\n",(0,r.jsxs)(t.li,{className:"task-list-item",children:[(0,r.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ","The episode must terminate successfully upon reaching a target of 20 workers."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h4,{id:"environment-design",children:(0,r.jsx)(t.strong,{children:"Environment Design"})}),"\n",(0,r.jsx)(t.p,{children:"To facilitate this learning, we will design a minimal environment with precisely defined observation, action, and reward structures."}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"1. Observation Space (The Agent's Input)"})}),"\n",(0,r.jsxs)(t.p,{children:['The state provided to the agent must contain only the information necessary to make the "build worker" decision. All values are normalized to a ',(0,r.jsx)(t.code,{children:"[0.0, 1.0]"})," range to improve neural network stability."]}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Index"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Feature"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Normalization Factor"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Purpose"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"0"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"self.minerals"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"1000"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:'"Can I afford the action?"'})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"1"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"self.workers.amount"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"50"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:'"How close am I to the goal?"'})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"2"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"self.supply_left"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"20"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:'"Do I have the capacity for a new unit?"'})]})]})]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"2. Action Space (The Agent's Output)"})}),"\n",(0,r.jsx)(t.p,{children:"The agent has a discrete, binary choice on each decision step."}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Action Value"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Agent's Intent"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"0"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Do Nothing"})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"1"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Attempt to build an SCV"})]})]})]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.strong,{children:"3. Reward Function (The Learning Signal)"})}),"\n",(0,r.jsx)(t.p,{children:"The reward function is engineered to provide clear and immediate feedback, guiding the agent toward the correct behavior."}),"\n",(0,r.jsxs)(t.table,{children:[(0,r.jsx)(t.thead,{children:(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Triggering Condition"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Reward Value"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Signal Type"}),(0,r.jsx)(t.th,{style:{textAlign:"left"},children:"Purpose"})]})}),(0,r.jsxs)(t.tbody,{children:[(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:'"Build" action is taken and is possible'}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"+5"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Sparse"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Strongly reinforce the correct decision."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:'"Build" action is taken but is impossible'}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"-5"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Sparse"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Strongly penalize impossible actions (teaches game rules)."})]}),(0,r.jsxs)(t.tr,{children:[(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Every decision step"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:(0,r.jsx)(t.code,{children:"+ (worker_count * 0.1)"})}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Dense"}),(0,r.jsx)(t.td,{style:{textAlign:"left"},children:"Continuously encourage progress toward the goal."})]})]})]}),"\n",(0,r.jsx)(t.hr,{}),"\n",(0,r.jsx)(t.h4,{id:"key-learning-challenges-for-the-agent",children:(0,r.jsx)(t.strong,{children:"Key Learning Challenges for the Agent"})}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Credit Assignment:"})," The agent must learn that the ",(0,r.jsx)(t.code,{children:"+5"}),' reward is directly caused by its "Build" action when resources are sufficient.']}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Constraint Learning:"})," The agent must learn to avoid the ",(0,r.jsx)(t.code,{children:"-5"})," penalty by paying attention to the ",(0,r.jsx)(t.code,{children:"minerals"})," and ",(0,r.jsx)(t.code,{children:"supply_left"})," observations, effectively learning the game's constraints."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.strong,{children:"Temporal Progression:"})," The dense reward for ",(0,r.jsx)(t.code,{children:"worker_count"}),' encourages the agent to not just "do nothing," but to actively pursue the goal over time.']}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"This tightly scoped problem provides a perfect, verifiable test case for our RL framework. In the next section, we will translate this design into code."})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>o});var i=n(6540);const r={},s=i.createContext(r);function l(e){const t=i.useContext(s);return i.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(s.Provider,{value:t},e.children)}}}]);