"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[3792],{4668:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.3 - Loading and Running a Trained Agent","title":"6.3 - Loading and Running a Trained Agent","description":"Training a model is only half the process. Once you have a trained agent saved to a file, you need a way to load and run it for evaluation, a process known as inference. This allows you to watch your agent perform, test its capabilities against specific opponents, and verify its learned policy without the overhead of retraining.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.3 - Loading and Running a Trained Agent.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.3 - Loading and Running a Trained Agent","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.3 - Loading and Running a Trained Agent","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"6.2 - A Quick Look at the PPO Algorithm","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.2 - A Quick Look at the PPO Algorithm"},"next":{"title":"7.1 - Setting up TensorBoard","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 7 - Visualizing and Evaluating Training/7.1 - Setting up TensorBoard"}}');var i=r(4848),o=r(8453);const s={},a=void 0,d={},l=[{value:"<strong>The Inference Workflow</strong>",id:"the-inference-workflow",level:4},{value:"<strong>Using the Learned Policy- <code>model.predict()</code></strong>",id:"using-the-learned-policy--modelpredict",level:4},{value:"<strong>Code- The Reusable <code>run_agent.py</code> Script</strong>",id:"code--the-reusable-run_agentpy-script",level:4}];function c(e){const n={code:"code",h4:"h4",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["Training a model is only half the process. Once you have a trained agent saved to a file, you need a way to load and run it for evaluation, a process known as ",(0,i.jsx)(n.strong,{children:"inference"}),". This allows you to watch your agent perform, test its capabilities against specific opponents, and verify its learned policy without the overhead of retraining."]}),"\n",(0,i.jsx)(n.p,{children:"This section provides a dedicated, reusable script for running inference on your saved models."}),"\n",(0,i.jsx)(n.h4,{id:"the-inference-workflow",children:(0,i.jsx)(n.strong,{children:"The Inference Workflow"})}),"\n",(0,i.jsx)(n.p,{children:"Unlike the training loop, the inference loop does not learn or update the model. It simply uses the model's existing policy to choose actions."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"+--------------------------+\r\n| 1. Select & Load Model   |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 2. Reset Environment     |\r\n+--------------------------+\r\n             |\r\n             v\r\n+--------------------------+\r\n| 3. Get Observation       |\r\n+--------------------------+\r\n             |\r\n             |       +-------------------+\r\n             |       | 4. Predict Action |\r\n             +------\x3e|   (model.predict) |\r\n                     +-------------------+\r\n             <------+\r\n             |\r\n             | 5. Step Environment\r\n             |\r\n             v\r\n+--------------------------+\r\n| 6. Repeat until Done     |\r\n+--------------------------+\n"})}),"\n",(0,i.jsx)(n.h4,{id:"using-the-learned-policy--modelpredict",children:(0,i.jsxs)(n.strong,{children:["Using the Learned Policy- ",(0,i.jsx)(n.code,{children:"model.predict()"})]})}),"\n",(0,i.jsxs)(n.p,{children:["The core of the inference loop is the ",(0,i.jsx)(n.code,{children:"model.predict()"})," method."]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Parameter"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Value"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Purpose"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"deterministic"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"True"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,i.jsx)(n.strong,{children:"(For Evaluation)"}),' Instructs the model to always choose the action with the highest probability. This reflects the agent\'s "true" learned policy and makes its behavior consistent.']})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"deterministic"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"False"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,i.jsx)(n.strong,{children:"(For Training)"})," Allows the model to sometimes sample from its action probabilities. This encourages exploration, which is necessary for learning but not for evaluation."]})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"code--the-reusable-run_agentpy-script",children:(0,i.jsxs)(n.strong,{children:["Code- The Reusable ",(0,i.jsx)(n.code,{children:"run_agent.py"})," Script"]})}),"\n",(0,i.jsx)(n.p,{children:"This script is your entry point for evaluating any of the agents you have trained."}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# run_agent.py\r\n\r\nimport multiprocessing as mp\r\nimport time\r\n\r\nfrom stable_baselines3 import PPO\r\n\r\n# Import all possible environments\r\nfrom worker_bot import WorkerEnv\r\nfrom macro_bot import MacroEnv\r\nfrom micro_bot import MicroEnv\r\n\r\n# --- Configuration ---\r\n# Use a dictionary to map names to environment classes and model paths.\r\nAGENT_CONFIG = {\r\n    "worker": {\r\n        "env": WorkerEnv,\r\n        "path": "./ppo_sc2_worker.zip"\r\n    },\r\n    "macro": {\r\n        "env": MacroEnv,\r\n        "path": "./ppo_sc2_macro.zip"\r\n    },\r\n    "micro": {\r\n        "env": MicroEnv,\r\n        "path": "./ppo_sc2_micro.zip"\r\n    }\r\n}\r\n# CHANGE THIS VALUE to select which agent to run\r\nSELECTED_AGENT = "micro"\r\n\r\ndef main():\r\n    """Loads and runs a pre-trained agent for one episode."""\r\n    \r\n    config = AGENT_CONFIG[SELECTED_AGENT]\r\n    print(f"--- Running evaluation for agent: {SELECTED_AGENT.upper()} ---")\r\n\r\n    # --- Step 1: Instantiate the Environment ---\r\n    env = config["env"]()\r\n\r\n    # --- Step 2: Load the Trained Model ---\r\n    try:\r\n        model = PPO.load(config["path"])\r\n        print(f"Model loaded successfully from: {config[\'path\']}")\r\n    except FileNotFoundError:\r\n        print(f"ERROR: Model file not found at {config[\'path\']}")\r\n        print("Please run train.py to train and save the model first.")\r\n        env.close()\r\n        return\r\n\r\n    # --- Step 3: Run the Inference Loop ---\r\n    obs, info = env.reset()\r\n    terminated = False\r\n    # Add a safety counter for truncated episodes\r\n    episode_steps = 0\r\n    max_episode_steps = 2000 # Approx 2.5 minutes of game time\r\n\r\n    while not terminated and episode_steps < max_episode_steps:\r\n        # Use the model to predict the next action.\r\n        # deterministic=True ensures the agent plays its best learned policy.\r\n        action, _states = model.predict(obs, deterministic=True)\r\n        \r\n        # Take the action in the environment.\r\n        obs, reward, terminated, truncated, info = env.step(action)\r\n        episode_steps += 1\r\n        \r\n        # Optional: Add a small delay to make it easier to watch.\r\n        time.sleep(0.01)\r\n\r\n    if episode_steps >= max_episode_steps:\r\n        print("Episode finished due to reaching max step limit (truncated).")\r\n    else:\r\n        print("Episode finished normally (terminated).")\r\n    \r\n    # --- Final Step: Clean up ---\r\n    env.close()\r\n\r\n\r\nif __name__ == \'__main__\':\r\n    # Add the multiprocessing guard for compatibility.\r\n    mp.freeze_support()\r\n    main()\n'})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>a});var t=r(6540);const i={},o=t.createContext(i);function s(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),t.createElement(o.Provider,{value:n},e.children)}}}]);