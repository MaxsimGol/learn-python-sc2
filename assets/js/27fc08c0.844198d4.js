"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[8990],{6097:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>s,default:()=>m,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation","title":"8.2 - Code - A Simplified DRL-RM Implementation","description":"This file provides a functional, albeit simplified, implementation of the \\"generalized agent\\" concept. It is designed to handle a variable number of units by encoding the game state into an entity list and using a decomposed action space.","source":"@site/docs/Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/02-Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation.md","sourceDirName":"Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/02-Chapter 8 - Advanced Implementation - DRL-RM","slug":"/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.2 - Code - A Simplified DRL-RM Implementation","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"8.1 - Concept - A More Generalized Agent","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.1 - Concept - A More Generalized Agent"},"next":{"title":"8.3 - The Challenge of a Large Action Space","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 8 - Advanced Implementation - DRL-RM/8.3 - The Challenge of a Large Action Space"}}');var i=t(4848),a=t(8453);const o={},s=void 0,d={},c=[{value:"<strong>The Code: <code>drl_rm_bot.py</code></strong>",id:"the-code-drl_rm_botpy",level:4}];function l(e){const n={code:"code",h4:"h4",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:'This file provides a functional, albeit simplified, implementation of the "generalized agent" concept. It is designed to handle a variable number of units by encoding the game state into an entity list and using a decomposed action space.'}),"\n",(0,i.jsx)(n.p,{children:"This code serves as a foundational example of how to structure an agent that can operate in a more complex and dynamic environment than our previous, specialized bots."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"the-code-drl_rm_botpy",children:(0,i.jsxs)(n.strong,{children:["The Code: ",(0,i.jsx)(n.code,{children:"drl_rm_bot.py"})]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# drl_rm_bot.py\r\n\r\nimport numpy as np\r\nimport gymnasium as gym\r\nfrom gymnasium.spaces import Box, MultiDiscrete\r\nimport queue\r\n\r\nfrom sc2.bot_ai import BotAI\r\nfrom sc2.race import Terran\r\nfrom sc2.ids.unit_typeid import UnitTypeId\r\nfrom sc2.units import Units\r\n# This is a hypothetical wrapper class you would create to bridge SC2 and Gymnasium.\r\n# Its implementation is not shown here but is assumed to exist.\r\nfrom sc2_gym_env import SC2GymEnv\r\n\r\n# --- Environment Constants ---\r\nMAX_UNITS = 50  # The maximum number of units to include in our observation\r\nNUM_UNIT_FEATURES = 5  # The number of features describing each unit\r\n# A high value for the observation space bound to accommodate various UnitTypeIds.\r\nMAX_UNIT_TYPE_ID = 2000\r\n\r\nclass DRL_RM_Bot(BotAI):\r\n    """\r\n    The BotAI for our simplified DRL-RM agent. It encodes the game state\r\n    into an entity list and decodes actions from the RL agent.\r\n    """\r\n    def __init__(self, action_queue, obs_queue):\r\n        super().__init__()\r\n        self.action_queue = action_queue\r\n        self.obs_queue = obs_queue\r\n        self.race = Terran\r\n\r\n    def _encode_state(self) -> np.ndarray:\r\n        """Encodes the current game state into a padded matrix of unit features."""\r\n        \r\n        # Combine our units and visible enemy units, sorted by tag for consistency.\r\n        all_units = (self.units + self.enemy_units).sorted(lambda u: u.tag)\r\n        \r\n        # Manually create a feature matrix.\r\n        encoded_units = np.zeros((len(all_units), NUM_UNIT_FEATURES), dtype=np.float32)\r\n\r\n        # A simple mapping of unit types to integers for the observation.\r\n        type_map = {UnitTypeId.MARINE: 1, UnitTypeId.SCV: 2, UnitTypeId.ZERGLING: 3}\r\n\r\n        for i, unit in enumerate(all_units):\r\n            map_width = self.game_info.map_size.width\r\n            map_height = self.game_info.map_size.height\r\n            \r\n            encoded_units[i] = [\r\n                type_map.get(unit.type_id, 0),  # Use 0 for unknown types\r\n                unit.health_percentage,\r\n                unit.position.x / map_width if map_width > 0 else 0,\r\n                unit.position.y / map_height if map_height > 0 else 0,\r\n                1.0 if unit.is_mine else 0.0  # Ownership feature\r\n            ]\r\n        \r\n        # Pad the observation with zeros up to MAX_UNITS.\r\n        obs = np.zeros((MAX_UNITS, NUM_UNIT_FEATURES), dtype=np.float32)\r\n        num_observed = min(len(encoded_units), MAX_UNITS)\r\n        if num_observed > 0:\r\n            obs[:num_observed] = encoded_units[:num_observed]\r\n            \r\n        return obs\r\n\r\n    def _decode_and_execute_action(self, action: list[int]):\r\n        """Decodes the agent\'s action and issues a command."""\r\n        actor_idx, ability_id, target_idx = action\r\n        \r\n        my_units = self.units.sorted(lambda u: u.tag)\r\n        # The list of all units must be sorted identically to _encode_state.\r\n        all_units = (self.units + self.enemy_units).sorted(lambda u: u.tag)\r\n\r\n        # --- Safety Checks ---\r\n        # Ensure the selected actor and target indices are valid.\r\n        if not (0 <= actor_idx < len(my_units)): return\r\n        if not (0 <= target_idx < min(len(all_units), MAX_UNITS)): return\r\n\r\n        actor = my_units[actor_idx]\r\n        target = all_units[target_idx]\r\n\r\n        # Ability 0: Move, Ability 1: Attack.\r\n        # Direct unit commands are used as per python-sc2 v7.0.5+.\r\n        if ability_id == 0:\r\n            actor.move(target.position)\r\n        elif ability_id == 1:\r\n            actor.attack(target)\r\n        \r\n    async def on_step(self, iteration: int):\r\n        # Issue commands on a regular interval to manage API overhead.\r\n        if iteration % 8 == 0:\r\n            # 1. DECODE AND EXECUTE ACTION from the agent.\r\n            try:\r\n                action = self.action_queue.get_nowait()\r\n                self._decode_and_execute_action(action)\r\n            except queue.Empty:\r\n                pass  # No action from agent in this step.\r\n            \r\n            # 2. ENCODE STATE AND SEND OBSERVATION to the agent.\r\n            observation = self._encode_state()\r\n            terminated = self.townhalls.amount == 0  # End if we lose all bases.\r\n            reward = 0.1  # A simple placeholder reward.\r\n            \r\n            # The obs queue expects a Gymnasium-style tuple.\r\n            self.obs_queue.put((observation, reward, terminated, False, {}))\r\n            \r\n            if terminated:\r\n                await self.client.leave()\r\n\r\n\r\nclass DRL_RM_Env(SC2GymEnv):\r\n    """The Gymnasium Wrapper for the DRL-RM bot."""\r\n    def __init__(self):\r\n        super().__init__(bot_class=DRL_RM_Bot, map_name="AcropolisLE")\r\n        \r\n        # Observation Space: A 2D matrix of (MAX_UNITS, NUM_FEATURES).\r\n        # The \'high\' value must accommodate unnormalized features like unit type IDs.\r\n        self.observation_space = Box(\r\n            low=0, high=MAX_UNIT_TYPE_ID,\r\n            shape=(MAX_UNITS, NUM_UNIT_FEATURES),\r\n            dtype=np.float32\r\n        )\r\n        \r\n        # Action Space: A vector of 3 integers for the decomposed action.\r\n        self.action_space = MultiDiscrete([\r\n            MAX_UNITS,    # Index of the friendly unit to act as the "actor"\r\n            2,            # Index of the ability to use (0=Move, 1=Attack)\r\n            MAX_UNITS     # Index of the target unit from the observed list\r\n        ])\n'})})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>s});var r=t(6540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);