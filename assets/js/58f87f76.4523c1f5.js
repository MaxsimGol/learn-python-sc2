"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[4094],{1408:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward","title":"3.2 - Design - Observation, Action, and Reward","description":"The performance and success of a reinforcement learning agent are dictated by the quality of its environment design. This document serves as the formal design specification for the three core components of our WorkerEnv: the Observation Space, the Action Space, and the Reward Function.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"3.1 - Goal - Learning a Single Economic Task","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.1 - Goal - Learning a Single Economic Task"},"next":{"title":"3.3 - Code - The Worker Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.3 - Code - The Worker Bot Environment"}}');var i=t(4848),s=t(8453);const o={},c=void 0,a={},l=[{value:"<strong>1. Observation Space Specification</strong>",id:"1-observation-space-specification",level:4},{value:"<strong>2. Action Space Specification</strong>",id:"2-action-space-specification",level:4},{value:"<strong>3. Reward Function Specification</strong>",id:"3-reward-function-specification",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["The performance and success of a reinforcement learning agent are dictated by the quality of its environment design. This document serves as the formal design specification for the three core components of our ",(0,i.jsx)(n.code,{children:"WorkerEnv"}),": the Observation Space, the Action Space, and the Reward Function."]}),"\n",(0,i.jsx)(n.h4,{id:"1-observation-space-specification",children:(0,i.jsx)(n.strong,{children:"1. Observation Space Specification"})}),"\n",(0,i.jsx)(n.p,{children:"The observation space defines the fixed-size vector of data that the environment provides to the agent on each step. It must contain all information necessary for the agent to make an informed decision for its given task."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gymnasium Type:"})," ",(0,i.jsx)(n.code,{children:"gymnasium.spaces.Box"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Shape:"})," ",(0,i.jsx)(n.code,{children:"(3,)"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Type:"})," ",(0,i.jsx)(n.code,{children:"numpy.float32"})]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Index"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Source Feature"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Normalization"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Purpose"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"0"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"self.minerals"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["Divide by ",(0,i.jsx)(n.code,{children:"1000.0"})]}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Informs the agent of its current purchasing power."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"1"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"self.workers.amount"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["Divide by ",(0,i.jsx)(n.code,{children:"50.0"})]}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Provides a metric of progress toward the episode's goal."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"2"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"self.supply_left"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["Divide by ",(0,i.jsx)(n.code,{children:"20.0"})]}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Signals whether the agent has the capacity for new units."})]})]})]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Design Rationale:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Minimalism:"}),' The state space is intentionally limited to only the features relevant to the "build worker" decision, focusing the learning process.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Normalization:"})," All inputs are scaled to a ",(0,i.jsx)(n.code,{children:"[0.0, 1.0]"})," range. This is a standard and critical practice for stabilizing neural network training."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"2-action-space-specification",children:(0,i.jsx)(n.strong,{children:"2. Action Space Specification"})}),"\n",(0,i.jsx)(n.p,{children:"The action space defines the set of all possible choices the agent can make. For this task, we use a simple discrete space."}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gymnasium Type:"})," ",(0,i.jsx)(n.code,{children:"gymnasium.spaces.Discrete"})]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Size:"})," ",(0,i.jsx)(n.code,{children:"2"})]}),"\n"]}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Action Value"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Agent's Intent"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"0"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,i.jsx)(n.strong,{children:"Do Nothing:"})," No command is issued."]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.code,{children:"1"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:[(0,i.jsx)(n.strong,{children:"Build Worker:"})," Attempt to train one SCV."]})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"3-reward-function-specification",children:(0,i.jsx)(n.strong,{children:"3. Reward Function Specification"})}),"\n",(0,i.jsx)(n.p,{children:"The reward function provides the learning signal to the agent. This design uses a combination of event-driven sparse rewards and state-based dense rewards."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Design Philosophy:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sparse Rewards for Correctness:"}),' Provide a large, immediate signal for "correct" or "incorrect" actions to quickly teach the rules and constraints of the environment.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Dense Rewards for Progress:"})," Provide a small, continuous signal to guide the agent toward the long-term goal and prevent passive, do-nothing behavior."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Pseudocode Logic:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:'function get_reward(action, game_state):\r\n  reward = 0\r\n\r\n  # Sparse Reward for the "Build Worker" action\r\n  if action == 1:\r\n    if game_state.can_train_worker:\r\n      reward += 5  // Correct action\r\n    else:\r\n      reward -= 5  // Wasted/Impossible action\r\n\r\n  # Dense Reward for progress\r\n  reward += game_state.worker_count * 0.1\r\n\r\n  return reward\n'})}),"\n",(0,i.jsx)(n.p,{children:"This hybrid reward structure is designed to train the agent efficiently, providing strong feedback for immediate decisions while maintaining a clear signal for the overall episode objective."})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>o,x:()=>c});var r=t(6540);const i={},s=r.createContext(i);function o(e){const n=r.useContext(s);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(s.Provider,{value:n},e.children)}}}]);