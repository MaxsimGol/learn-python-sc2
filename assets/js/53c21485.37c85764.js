"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[6926],{3802:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.2 - Example - A Curriculum for the Micro Bot","title":"10.2 - Example - A Curriculum for the Micro Bot","description":"This section provides a practical, code-oriented example of how to implement a curriculum. We will design a simple, three-stage program to teach our MicroBot agent how to handle progressively more difficult combat scenarios, starting with the foundational 1v1 kiting skill.","source":"@site/docs/Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/04-Chapter 10 - Improving Your Agent with Curriculum Learning/10.2 - Example - A Curriculum for the Micro Bot.md","sourceDirName":"Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/04-Chapter 10 - Improving Your Agent with Curriculum Learning","slug":"/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.2 - Example - A Curriculum for the Micro Bot","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.2 - Example - A Curriculum for the Micro Bot","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"10.1 - The Concept - From Simple to Complex Tasks","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks"},"next":{"title":"11.1 - Summary of Your RL Journey","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 11 - Conclusion and Further Exploration/11.1 - Summary of Your RL Journey"}}');var i=n(4848),a=n(8453);const s={},o=void 0,l={},c=[{value:"<strong>Curriculum Specification</strong>",id:"curriculum-specification",level:4},{value:"<strong>Step 1- Parameterize the <code>gymnasium</code> Environment</strong>",id:"step-1--parameterize-the-gymnasium-environment",level:4},{value:"<strong>Step 2- The Automated Training Script (Pseudocode)</strong>",id:"step-2--the-automated-training-script-pseudocode",level:4}];function d(e){const t={code:"code",h4:"h4",hr:"hr",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(t.p,{children:["This section provides a practical, code-oriented example of how to implement a curriculum. We will design a simple, three-stage program to teach our ",(0,i.jsx)(t.code,{children:"MicroBot"})," agent how to handle progressively more difficult combat scenarios, starting with the foundational 1v1 kiting skill."]}),"\n",(0,i.jsx)(t.h4,{id:"curriculum-specification",children:(0,i.jsx)(t.strong,{children:"Curriculum Specification"})}),"\n",(0,i.jsx)(t.p,{children:"A formal training plan requires defining the stages, the skill taught at each stage, and the criteria for advancing to the next."}),"\n",(0,i.jsxs)(t.table,{children:[(0,i.jsx)(t.thead,{children:(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.th,{style:{textAlign:"left"},children:"Stage"}),(0,i.jsx)(t.th,{style:{textAlign:"left"},children:"Task Name"}),(0,i.jsx)(t.th,{style:{textAlign:"left"},children:"Scenario"}),(0,i.jsx)(t.th,{style:{textAlign:"left"},children:"Primary Skill Learned"}),(0,i.jsx)(t.th,{style:{textAlign:"left"},children:"Graduation Criterion"})]})}),(0,i.jsxs)(t.tbody,{children:[(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{style:{textAlign:"left"},children:(0,i.jsx)(t.strong,{children:"1"})}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"Kiting Fundamentals"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"1 Marine vs. 1 Zergling"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"The core mechanic of kiting: move during weapon cooldown."}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"> 95% win rate vs. built-in AI."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{style:{textAlign:"left"},children:(0,i.jsx)(t.strong,{children:"2"})}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"Target Prioritization"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"2 Marines vs. 1 Zergling"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"How to focus fire on a single target with multiple units."}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"> 98% win rate vs. built-in AI."})]}),(0,i.jsxs)(t.tr,{children:[(0,i.jsx)(t.td,{style:{textAlign:"left"},children:(0,i.jsx)(t.strong,{children:"3"})}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"Group Engagement"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"2 Marines vs. 2 Zerglings"}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"How to manage multiple threats and maintain group cohesion."}),(0,i.jsx)(t.td,{style:{textAlign:"left"},children:"> 90% win rate vs. built-in AI."})]})]})]}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h4,{id:"step-1--parameterize-the-gymnasium-environment",children:(0,i.jsxs)(t.strong,{children:["Step 1- Parameterize the ",(0,i.jsx)(t.code,{children:"gymnasium"})," Environment"]})}),"\n",(0,i.jsxs)(t.p,{children:["The first step is to modify our ",(0,i.jsx)(t.code,{children:"MicroEnv"})," so that a single class can create any of the scenarios defined in our curriculum. This is achieved by adding parameters to its constructor."]}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsxs)(t.strong,{children:["Revised ",(0,i.jsx)(t.code,{children:"micro_bot.py"})," (Key Modifications):"]})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'# In micro_bot.py\r\n\r\nclass MicroBot(BotAI):\r\n    # The BotAI class now accepts the scenario configuration as arguments\r\n    def __init__(self, num_marines: int, num_zerglings: int, **kwargs):\r\n        super().__init__(**kwargs)\r\n        self.num_marines = num_marines\r\n        self.num_zerglings = num_zerglings\r\n        # ... rest of __init__ ...\r\n\r\n    async def on_start(self):\r\n        # The on_start method uses these parameters to create the correct scenario\r\n        p = self.start_location\r\n        await self._client.debug_create_unit([[UnitTypeId.MARINE, self.num_marines, p, 1]])\r\n        await self._client.debug_create_unit([[UnitTypeId.ZERGLING, self.num_zerglings, p.towards(self.enemy_start_locations[0], 8), 2]])\r\n        await self._client.debug_kill_unit(self.workers)\r\n\r\nclass MicroEnv(SC2GymEnv):\r\n    # The GymEnv now accepts the same parameters to pass them on to the bot\r\n    def __init__(self, num_marines: int = 1, num_zerglings: int = 1):\r\n        \r\n        # This is a critical step: Since the BotAI class needs arguments,\r\n        # we must wrap its constructor in a lambda that our SC2GymEnv can call.\r\n        # This passes the correct arguments when the bot is instantiated\r\n        # inside the separate game process.\r\n        bot_constructor = lambda **kwargs: MicroBot(\r\n            num_marines=num_marines,\r\n            num_zerglings=num_zerglings,\r\n            **kwargs\r\n        )\r\n        \r\n        super().__init__(bot_class=bot_constructor, map_name="AcropolisLE")\r\n        # ... rest of __init__ ...\n'})}),"\n",(0,i.jsx)(t.hr,{}),"\n",(0,i.jsx)(t.h4,{id:"step-2--the-automated-training-script-pseudocode",children:(0,i.jsx)(t.strong,{children:"Step 2- The Automated Training Script (Pseudocode)"})}),"\n",(0,i.jsx)(t.p,{children:"With a configurable environment, we can now write a training manager script that automates the curriculum. This script is responsible for creating the environment for each stage, training the agent, evaluating its performance, and loading its learned policy into the next stage."}),"\n",(0,i.jsx)(t.p,{children:"The following pseudocode outlines the required logic."}),"\n",(0,i.jsx)(t.p,{children:(0,i.jsxs)(t.strong,{children:[(0,i.jsx)(t.code,{children:"curriculum_trainer.py"})," (Pseudocode Logic):"]})}),"\n",(0,i.jsx)(t.pre,{children:(0,i.jsx)(t.code,{className:"language-python",children:'from sb3_contrib import MaskablePPO\r\nfrom micro_bot import MicroEnv\r\nfrom evaluation import evaluate_agent # Assume a helper function for evaluation\r\n\r\n# --- Define the Curriculum Stages ---\r\ncurriculum = [\r\n    {"name": "1v1", "marines": 1, "zerglings": 1, "win_rate_goal": 0.95, "model_save_path": "model_stage1.zip"},\r\n    {"name": "2v1", "marines": 2, "zerglings": 1, "win_rate_goal": 0.98, "model_save_path": "model_stage2.zip"},\r\n    {"name": "2v2", "marines": 2, "zerglings": 2, "win_rate_goal": 0.90, "model_save_path": "final_micro_model.zip"},\r\n]\r\n\r\ndef run_training_curriculum():\r\n    last_model_path = None\r\n    \r\n    for stage in curriculum:\r\n        print(f"--- Starting Curriculum Stage: {stage[\'name\']} ---")\r\n        \r\n        # 1. Create the environment for the current stage\r\n        env = MicroEnv(num_marines=stage["marines"], num_zerglings=stage["zerglings"])\r\n        \r\n        # 2. Load the model from the previous stage, or create a new one for the first stage\r\n        if last_model_path:\r\n            model = MaskablePPO.load(last_model_path, env=env)\r\n            print(f"Loaded model from {last_model_path}")\r\n        else:\r\n            model = MaskablePPO("MlpPolicy", env, verbose=1)\r\n            print("Creating new model for the first stage.")\r\n\r\n        # 3. Train the model until it meets the graduation criterion\r\n        while evaluate_agent(model, env) < stage["win_rate_goal"]:\r\n            model.learn(total_timesteps=25_000) # Train for a fixed number of steps per loop\r\n            \r\n        # 4. Save the new model and prepare for the next stage\r\n        model.save(stage["model_save_path"])\r\n        last_model_path = stage["model_save_path"]\r\n        print(f"--- Stage {stage[\'name\']} Complete. Model saved to {last_model_path} ---")\r\n        env.close()\r\n\r\nif __name__ == \'__main__\':\r\n    run_training_curriculum()\n'})}),"\n",(0,i.jsx)(t.p,{children:"This structured approach provides a clear and powerful method for tackling complex RL problems. By building upon previously learned skills, the agent can achieve a level of performance that would be difficult or impossible to reach with a single, monolithic training run."})]})}function m(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>s,x:()=>o});var r=n(6540);const i={},a=r.createContext(i);function s(e){const t=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:t},e.children)}}}]);