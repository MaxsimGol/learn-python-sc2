"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[3553],{1650:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2","title":"1.1 - What is RL for SC2","description":"The documentation so far has focused on building scripted agents. In that paradigm, you, the developer, explicitly write the rules (if/then/else logic) that determine the bot\'s behavior. Reinforcement Learning (RL) introduces a fundamentally different approach: instead of writing the rules, you define a goal, and the agent learns its own rules through trial and error.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.1 - What is RL for SC2","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"9.3 - Learning From Other Bots","permalink":"/learn-python-sc2/docs/Part 4 - The Wider World/Chapter 9 - Community and Resources/9.3 - Learning From Other Bots"},"next":{"title":"1.2 - The Core Challenge - async vs sync","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 1 - Introduction to RL Concepts/1.2 - The Core Challenge - async vs sync"}}');var s=n(4848),i=n(8453);const l={},o=void 0,a={},d=[{value:"<strong>Two Paradigms- Scripted vs. Learned</strong>",id:"two-paradigms--scripted-vs-learned",level:4},{value:"<strong>The Core RL Workflow</strong>",id:"the-core-rl-workflow",level:4},{value:"<strong>Key Terminology</strong>",id:"key-terminology",level:4},{value:"<strong>Our Goal for This Section</strong>",id:"our-goal-for-this-section",level:4},{value:"<strong>Why is RL Challenging in StarCraft II?</strong>",id:"why-is-rl-challenging-in-starcraft-ii",level:4}];function c(e){const t={code:"code",h4:"h4",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsxs)(t.p,{children:["The documentation so far has focused on building ",(0,s.jsx)(t.strong,{children:"scripted agents"}),". In that paradigm, you, the developer, explicitly write the rules (",(0,s.jsx)(t.code,{children:"if/then/else"})," logic) that determine the bot's behavior. Reinforcement Learning (RL) introduces a fundamentally different approach: instead of writing the rules, you define a ",(0,s.jsx)(t.strong,{children:"goal"}),", and the agent learns its own rules through trial and error."]}),"\n",(0,s.jsxs)(t.p,{children:["This section will guide you through building a ",(0,s.jsx)(t.strong,{children:"learned agent"}),', where the "brain" of the bot is a neural network model trained using libraries like ',(0,s.jsx)(t.code,{children:"stable-baselines3"}),"."]}),"\n",(0,s.jsx)(t.h4,{id:"two-paradigms--scripted-vs-learned",children:(0,s.jsx)(t.strong,{children:"Two Paradigms- Scripted vs. Learned"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Feature"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Scripted Agent (Traditional Bot)"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Learned Agent (RL Bot)"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Developer's Role"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Write explicit rules for every scenario."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Design the learning environment, observation, actions, and reward signal."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Decision Making"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Based on a pre-defined logic tree."}),(0,s.jsxs)(t.td,{style:{textAlign:"left"},children:["Based on a learned ",(0,s.jsx)(t.strong,{children:"policy"})," that maps observations to actions."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Behavior"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Predictable and consistent."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Can be creative and discover non-obvious strategies."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Primary Challenge"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Anticipating all possible game states."}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"Designing a reward function that leads to desired behavior; long training times."})]})]})]}),"\n",(0,s.jsx)(t.h4,{id:"the-core-rl-workflow",children:(0,s.jsx)(t.strong,{children:"The Core RL Workflow"})}),"\n",(0,s.jsxs)(t.p,{children:["At its heart, RL is a feedback loop between an ",(0,s.jsx)(t.strong,{children:"Agent"})," and an ",(0,s.jsx)(t.strong,{children:"Environment"}),"."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"          +------------------+           +------------------+\r\n          |                  |-----------|                  |\r\n          |      Agent       |  Action   |   Environment    |\r\n          | (Your RL Model)  |----------\x3e|  (StarCraft II)  |\r\n          |                  |           |                  |\r\n          +-------^----------+           +--------v---------+\r\n                  |                                |\r\n                  |  Observation + Reward          |\r\n                  +--------------------------------+\n"})}),"\n",(0,s.jsxs)(t.ol,{children:["\n",(0,s.jsxs)(t.li,{children:["The ",(0,s.jsx)(t.strong,{children:"Agent"})," observes the state of the ",(0,s.jsx)(t.strong,{children:"Environment"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["Based on the observation, the Agent chooses an ",(0,s.jsx)(t.strong,{children:"Action"}),"."]}),"\n",(0,s.jsxs)(t.li,{children:["The Environment updates based on the Action and returns a new ",(0,s.jsx)(t.strong,{children:"Observation"})," and a ",(0,s.jsx)(t.strong,{children:"Reward"})," signal."]}),"\n",(0,s.jsxs)(t.li,{children:["The Agent uses the Reward to update its internal strategy (its ",(0,s.jsx)(t.strong,{children:"Policy"}),") to take better actions in the future."]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"key-terminology",children:(0,s.jsx)(t.strong,{children:"Key Terminology"})}),"\n",(0,s.jsxs)(t.table,{children:[(0,s.jsx)(t.thead,{children:(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Term"}),(0,s.jsx)(t.th,{style:{textAlign:"left"},children:"Definition in our SC2 Context"})]})}),(0,s.jsxs)(t.tbody,{children:[(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Agent"})}),(0,s.jsxs)(t.td,{style:{textAlign:"left"},children:["The ",(0,s.jsx)(t.code,{children:"stable-baselines3"}),' model we will train. This is the "brain."']})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Environment"})}),(0,s.jsxs)(t.td,{style:{textAlign:"left"},children:["The StarCraft II game, wrapped in a custom ",(0,s.jsx)(t.code,{children:"gymnasium"})," interface."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Observation"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"A numerical representation of the game state (e.g., an array with minerals, supply, unit health)."})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Action"})}),(0,s.jsxs)(t.td,{style:{textAlign:"left"},children:["A discrete choice the Agent makes (e.g., ",(0,s.jsx)(t.code,{children:"0"}),' for "do nothing," ',(0,s.jsx)(t.code,{children:"1"}),' for "build worker").']})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Reward"})}),(0,s.jsxs)(t.td,{style:{textAlign:"left"},children:["A numerical score (",(0,s.jsx)(t.code,{children:"+1"}),", ",(0,s.jsx)(t.code,{children:"-10"}),", etc.) we give the Agent to reinforce good behavior and punish bad behavior."]})]}),(0,s.jsxs)(t.tr,{children:[(0,s.jsx)(t.td,{style:{textAlign:"left"},children:(0,s.jsx)(t.strong,{children:"Policy"})}),(0,s.jsx)(t.td,{style:{textAlign:"left"},children:"The internal strategy of the Agent; essentially, a function that decides which action to take for a given observation."})]})]})]}),"\n",(0,s.jsx)(t.h4,{id:"our-goal-for-this-section",children:(0,s.jsx)(t.strong,{children:"Our Goal for This Section"})}),"\n",(0,s.jsxs)(t.p,{children:["Our project is to build the bridge between ",(0,s.jsx)(t.code,{children:"python-sc2"})," and ",(0,s.jsx)(t.code,{children:"stable-baselines3"})," and train agents to complete specific tasks."]}),"\n",(0,s.jsxs)(t.ul,{className:"contains-task-list",children:["\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,s.jsx)(t.strong,{children:"Task 1:"})," Create a reusable ",(0,s.jsx)(t.code,{children:"gymnasium"})," environment for StarCraft II."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,s.jsx)(t.strong,{children:"Task 2:"})," Train an agent to perform a simple economic task."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,s.jsx)(t.strong,{children:"Task 3:"})," Train an agent to make a simple macroeconomic trade-off."]}),"\n",(0,s.jsxs)(t.li,{className:"task-list-item",children:[(0,s.jsx)(t.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,s.jsx)(t.strong,{children:"Task 4:"})," Train an agent to perform a simple combat micro task."]}),"\n"]}),"\n",(0,s.jsx)(t.h4,{id:"why-is-rl-challenging-in-starcraft-ii",children:(0,s.jsx)(t.strong,{children:"Why is RL Challenging in StarCraft II?"})}),"\n",(0,s.jsx)(t.p,{children:"Applying RL to a complex game like StarCraft II is difficult for two main reasons:"}),"\n",(0,s.jsxs)(t.ul,{children:["\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Massive State Space:"})," The number of possible game states is nearly infinite. We must carefully select a small subset of features for our agent to observe."]}),"\n",(0,s.jsxs)(t.li,{children:[(0,s.jsx)(t.strong,{children:"Credit Assignment Problem:"})," A game can last for minutes. If you win, was it because of a good decision you made 30 seconds ago or 10 minutes ago? Designing reward functions that correctly assign credit is a major challenge."]}),"\n"]}),"\n",(0,s.jsx)(t.p,{children:"In the following chapters, we will address these challenges by starting with very simple, focused tasks."})]})}function h(e={}){const{wrapper:t}={...(0,i.R)(),...e.components};return t?(0,s.jsx)(t,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}},8453:(e,t,n)=>{n.d(t,{R:()=>l,x:()=>o});var r=n(6540);const s={},i=r.createContext(s);function l(e){const t=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:l(e.components),r.createElement(i.Provider,{value:t},e.children)}}}]);