"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[2721],{60:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.3 - Code - The Worker Bot Environment","title":"3.3 - Code - The Worker Bot Environment","description":"This file provides the complete implementation for our first reinforcement learning task. It contains two key classes: WorkerBot, the BotAI that executes within the game, and WorkerEnv, the gymnasium.Env that provides the interface to the stable-baselines3 agent.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.3 - Code - The Worker Bot Environment.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.3 - Code - The Worker Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.3 - Code - The Worker Bot Environment","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"3.2 - Design - Observation, Action, and Reward","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 3 - The Worker Bot/3.2 - Design - Observation, Action, and Reward"},"next":{"title":"4.1 - Goal - Learning an Economic Trade-off","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.1 - Goal - Learning an Economic Trade-off"}}');var o=t(4848),i=t(8453);const s={},a=void 0,c={},l=[{value:"<strong>Implementation Overview</strong>",id:"implementation-overview",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsxs)(n.p,{children:["This file provides the complete implementation for our first reinforcement learning task. It contains two key classes: ",(0,o.jsx)(n.code,{children:"WorkerBot"}),", the ",(0,o.jsx)(n.code,{children:"BotAI"})," that executes within the game, and ",(0,o.jsx)(n.code,{children:"WorkerEnv"}),", the ",(0,o.jsx)(n.code,{children:"gymnasium.Env"})," that provides the interface to the ",(0,o.jsx)(n.code,{children:"stable-baselines3"})," agent."]}),"\n",(0,o.jsx)(n.p,{children:"This code is a direct translation of the design specification from the previous section."}),"\n",(0,o.jsx)(n.h4,{id:"implementation-overview",children:(0,o.jsx)(n.strong,{children:"Implementation Overview"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsxs)(n.strong,{children:[(0,o.jsx)(n.code,{children:"WorkerBot"})," (The Game Actor):"]})}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Initializes with communication queues."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","On a throttled loop (",(0,o.jsx)(n.code,{children:"iteration % 8"}),"), it blocks, waiting for an action from the agent."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Executes the received action."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Calculates the reward and the next observation."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Puts the ",(0,o.jsx)(n.code,{children:"(obs, reward, terminated, ...)"})," tuple back on the queue for the agent."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Manages the episode termination condition."]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsxs)(n.strong,{children:[(0,o.jsx)(n.code,{children:"WorkerEnv"})," (The Environment Interface):"]})}),"\n",(0,o.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Inherits from our reusable ",(0,o.jsx)(n.code,{children:"SC2GymEnv"}),"."]}),"\n",(0,o.jsxs)(n.li,{className:"task-list-item",children:[(0,o.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Formally defines the ",(0,o.jsx)(n.code,{children:"action_space"})," and ",(0,o.jsx)(n.code,{children:"observation_space"})," to match our design."]}),"\n"]}),"\n",(0,o.jsx)(n.hr,{}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.code,{children:"worker_bot.py"})})}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nfrom gymnasium.spaces import Box, Discrete\r\nimport multiprocessing as mp\r\nfrom queue import Empty\r\n\r\nfrom burnysc2.bot_ai import BotAI\r\nfrom burnysc2.ids.unit_typeid import UnitTypeId\r\nfrom sc2_gym_env import SC2GymEnv, ObservationQueueItem\r\n\r\n# --- The BotAI Implementation (Runs in the Game Process) ---\r\n\r\nclass WorkerBot(BotAI):\r\n    """\r\n    The BotAI actor that executes actions and generates observations.\r\n    Its only goal is to learn the correct policy for building workers.\r\n    """\r\n    def __init__(self, action_queue: mp.Queue, obs_queue: mp.Queue[ObservationQueueItem]):\r\n        super().__init__()\r\n        self.action_queue = action_queue\r\n        self.obs_queue = obs_queue\r\n\r\n    def _handle_action(self, action: int) -> float:\r\n        """\r\n        Executes the agent\'s action and calculates the sparse reward.\r\n        Returns the reward for the action.\r\n        """\r\n        can_afford_scv = self.can_afford(UnitTypeId.SCV)\r\n        has_idle_cc = self.townhalls.idle.exists\r\n\r\n        if action == 1:  # Action: Build SCV\r\n            if can_afford_scv and has_idle_cc:\r\n                self.train(UnitTypeId.SCV)\r\n                return 5.0  # Positive reward for a correct action\r\n            else:\r\n                return -5.0  # Negative reward for an impossible/wasted action\r\n        return 0.0  # No reward for "Do Nothing" action\r\n\r\n    async def on_step(self, iteration: int):\r\n        """\r\n        The main game loop, throttled to interact with the agent every 8 steps.\r\n        """\r\n        if iteration % 8 != 0:\r\n            return\r\n\r\n        try:\r\n            # 1. GET ACTION - This is a blocking call, waiting for the RL agent\r\n            action = self.action_queue.get(timeout=1)\r\n\r\n            # 2. EXECUTE ACTION & GET SPARSE REWARD\r\n            reward = self._handle_action(action)\r\n\r\n            # 3. ADD DENSE REWARD for progress\r\n            reward += self.workers.amount * 0.1\r\n\r\n            # 4. GET OBSERVATION for the next state\r\n            observation = np.array([\r\n                self.minerals / 1000.0,\r\n                self.workers.amount / 50.0,\r\n                self.supply_left / 20.0\r\n            ], dtype=np.float32)\r\n\r\n            # 5. DEFINE TERMINATION CONDITION\r\n            terminated = self.workers.amount >= 20\r\n\r\n            # 6. SEND DATA - Put the results on the queue for the RL agent\r\n            self.obs_queue.put((observation, reward, terminated, False, {}))\r\n\r\n            if terminated:\r\n                await self.client.leave()\r\n\r\n        except Empty:\r\n            # This can happen if the training process is killed.\r\n            print("Action queue was empty. Assuming training has ended.")\r\n            await self.client.leave()\r\n            return\r\n\r\n\r\n# --- The Gymnasium Environment (Runs in the Main Process) ---\r\n\r\nclass WorkerEnv(SC2GymEnv):\r\n    """\r\n    The Gymnasium Wrapper for the WorkerBot.\r\n    \r\n    This class defines the action and observation spaces that are visible\r\n    to the stable-baselines3 agent.\r\n    """\r\n    def __init__(self):\r\n        # Pass our custom BotAI class and a map name to the parent.\r\n        super().__init__(bot_class=WorkerBot, map_name="AcropolisLE")\r\n        \r\n        # The agent can choose between two actions: 0 or 1.\r\n        self.action_space = Discrete(2)\r\n        \r\n        # The observation is a 1D array of 3 normalized float values.\r\n        self.observation_space = Box(\r\n            low=0.0,\r\n            high=np.inf,\r\n            shape=(3,),\r\n            dtype=np.float32\r\n        )\n'})})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const o={},i=r.createContext(o);function s(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:s(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);