"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[4204],{8453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>s});var t=r(6540);const i={},o=t.createContext(i);function a(e){const n=t.useContext(o);return t.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(o.Provider,{value:n},e.children)}},9589:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>l,contentTitle:()=>s,default:()=>m,frontMatter:()=>a,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.3 - Code - The Micro Bot Environment","title":"5.3 - Code - The Micro Bot Environment","description":"This file, micro_bot.py, provides the complete implementation for our combat micro-management task. It defines the MicroBot and MicroEnv classes, which create a controlled 1v1 kiting scenario for our agent to learn in.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.3 - Code - The Micro Bot Environment.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.3 - Code - The Micro Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.3 - Code - The Micro Bot Environment","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"5.2 - Design - Observations for Spatial Awareness","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.2 - Design - Observations for Spatial Awareness"},"next":{"title":"6.1 - Code - The Reusable train.py Script","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 6 - Training Your Agents/6.1 - Code - The Reusable train.py Script"}}');var i=r(4848),o=r(8453);const a={},s=void 0,l={},c=[{value:"<strong>The Code: <code>micro_bot.py</code></strong>",id:"the-code-micro_botpy",level:4}];function f(e){const n={code:"code",h4:"h4",hr:"hr",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["This file, ",(0,i.jsx)(n.code,{children:"micro_bot.py"}),", provides the complete implementation for our combat micro-management task. It defines the ",(0,i.jsx)(n.code,{children:"MicroBot"})," and ",(0,i.jsx)(n.code,{children:"MicroEnv"})," classes, which create a controlled 1v1 kiting scenario for our agent to learn in."]}),"\n",(0,i.jsxs)(n.p,{children:["This code is the final translation of our combat design specification into a functional ",(0,i.jsx)(n.code,{children:"gymnasium"})," environment."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"the-code-micro_botpy",children:(0,i.jsxs)(n.strong,{children:["The Code: ",(0,i.jsx)(n.code,{children:"micro_bot.py"})]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# micro_bot.py\r\n\r\nimport numpy as np\r\nimport gymnasium as gym\r\nfrom gymnasium.spaces import Box, Discrete\r\nimport queue  # For handling queue empty exceptions\r\n\r\nfrom sc2.bot_ai import BotAI\r\nfrom sc2.ids.unit_typeid import UnitTypeId\r\nfrom sc2.unit import Unit\r\nfrom sc2_gym_env import SC2GymEnv  # Import our custom wrapper\r\n\r\n\r\nclass MicroBot(BotAI):\r\n    """\r\n    The BotAI actor for learning to kite in a controlled 1v1 scenario.\r\n    """\r\n\r\n    def __init__(self, action_queue, obs_queue):\r\n        super().__init__()\r\n        self.action_queue = action_queue\r\n        self.obs_queue = obs_queue\r\n\r\n        # State variables for tracking the combatants and their health\r\n        self.marine_tag: int = 0\r\n        self.zergling_tag: int = 0\r\n        self.last_marine_hp: float = 0\r\n        self.last_zergling_hp: float = 0\r\n\r\n    async def on_start(self):\r\n        """\r\n        Uses debug commands to create the 1v1 Marine vs. Zergling scenario.\r\n        """\r\n        p = self.start_location\r\n        await self.client.debug_create_unit([[UnitTypeId.MARINE, 1, p, 1]])\r\n        await self.client.debug_create_unit([[UnitTypeId.ZERGLING, 1, p.towards(self.enemy_start_locations[0], 8), 2]])\r\n        await self.client.debug_kill_unit(self.workers)\r\n\r\n    def _get_units(self) -> tuple[Unit | None, Unit | None]:\r\n        """Helper to retrieve the current Unit objects for our combatants."""\r\n        marine = self.units.find_by_tag(self.marine_tag)\r\n        zergling = self.enemy_units.find_by_tag(self.zergling_tag)\r\n        return marine, zergling\r\n\r\n    async def _handle_terminal_state(self, marine_won: bool):\r\n        """Sends the final reward and terminates the episode."""\r\n        reward = 100.0 if marine_won else -100.0\r\n        # Send final observation (an array of zeros)\r\n        self.obs_queue.put((np.zeros(5, dtype=np.float32), reward, True, False, {}))\r\n        await self.client.leave()\r\n\r\n    async def on_step(self, iteration: int):\r\n        # On the very first step, find the units and initialize their tags/health.\r\n        if iteration == 0:\r\n            if self.units and self.enemy_units:\r\n                self.marine_tag = self.units.first.tag\r\n                self.zergling_tag = self.enemy_units.first.tag\r\n                self.last_marine_hp = self.units.first.health_max\r\n                self.last_zergling_hp = self.enemy_units.first.health_max\r\n\r\n        marine, zergling = self._get_units()\r\n\r\n        # Check for a terminal state (win/loss).\r\n        if not marine or not zergling:\r\n            await self._handle_terminal_state(marine_won=(marine is not None))\r\n            return\r\n\r\n        # Throttle the RL loop.\r\n        if iteration % 4 == 0:\r\n            try:\r\n                action = self.action_queue.get_nowait()\r\n\r\n                # Execute the agent\'s chosen maneuver.\r\n                if action == 0:\r\n                    self.do(marine.attack(zergling))\r\n                elif action == 1:\r\n                    self.do(marine.move(marine.position.towards(zergling.position, -2)))\r\n                elif action == 2:\r\n                    self.do(marine.move(zergling.position))\r\n\r\n                # Calculate the health differential reward.\r\n                marine_hp, zergling_hp = marine.health, zergling.health\r\n                reward = (self.last_zergling_hp - zergling_hp) - (self.last_marine_hp - marine_hp)\r\n                self.last_marine_hp, self.last_zergling_hp = marine_hp, zergling_hp\r\n\r\n                # Create the observation vector for the new state.\r\n                observation = np.array([\r\n                    marine.health_percentage,\r\n                    1.0 if marine.weapon_cooldown > 0 else 0.0,\r\n                    zergling.health_percentage,\r\n                    marine.distance_to(zergling) / 15.0,  # Normalize distance\r\n                    1.0 if marine.is_attacking and marine.target_in_range(zergling) else 0.0,\r\n                ], dtype=np.float32)\r\n\r\n                self.obs_queue.put((observation, reward, False, False, {}))\r\n\r\n            except queue.Empty:\r\n                return  # No action from agent, continue.\r\n\r\n\r\nclass MicroEnv(SC2GymEnv):\r\n    """\r\n    The Gymnasium environment for our MicroBot.\r\n    """\r\n\r\n    def __init__(self):\r\n        super().__init__(bot_class=MicroBot, map_name="AcropolisLE")\r\n\r\n        # Action space: 3 discrete combat maneuvers\r\n        self.action_space = Discrete(3)\r\n\r\n        # Observation space: A 5-element vector of combat data\r\n        low = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)\r\n        high = np.array([1.0, 1.0, 1.0, np.inf, 1.0], dtype=np.float32)\r\n        self.observation_space = Box(\r\n            low=low,\r\n            high=high,\r\n            dtype=np.float32\r\n        )\n'})})]})}function m(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(f,{...e})}):f(e)}}}]);