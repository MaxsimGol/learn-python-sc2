"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[8733],{1372:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>p,frontMatter:()=>s,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.3 - Code - The Macro Bot Environment","title":"4.3 - Code - The Macro Bot Environment","description":"This file contains the complete implementation of the MacroEnv. It builds directly upon the concepts from the WorkerBot but expands the agent\'s capabilities to include managing supply, forcing it to learn a more complex economic policy.","source":"@site/docs/Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.3 - Code - The Macro Bot Environment.md","sourceDirName":"Part 5 - Reinforcement Learning/1 - Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot","slug":"/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.3 - Code - The Macro Bot Environment","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.3 - Code - The Macro Bot Environment","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"4.2 - Design - Expanding the State and Action Space","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 4 - The Macro Bot/4.2 - Design - Expanding the State and Action Space"},"next":{"title":"5.1 - Goal - Learning a Combat Skill - Kiting","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Foundations of Reinforcement Learning/Chapter 5 - The Micro Bot/5.1 - Goal - Learning a Combat Skill - Kiting"}}');var i=t(4848),o=t(8453);const s={},a=void 0,c={},l=[{value:"<strong>Implementation Checklist</strong>",id:"implementation-checklist",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",input:"input",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsxs)(n.p,{children:["This file contains the complete implementation of the ",(0,i.jsx)(n.code,{children:"MacroEnv"}),". It builds directly upon the concepts from the ",(0,i.jsx)(n.code,{children:"WorkerBot"})," but expands the agent's capabilities to include managing supply, forcing it to learn a more complex economic policy."]}),"\n",(0,i.jsxs)(n.p,{children:["This code translates the design specification from the preceding sections into a functional ",(0,i.jsx)(n.code,{children:"gymnasium"})," environment."]}),"\n",(0,i.jsx)(n.h4,{id:"implementation-checklist",children:(0,i.jsx)(n.strong,{children:"Implementation Checklist"})}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"MacroBot"})," (",(0,i.jsx)(n.code,{children:"BotAI"})," subclass):"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Handles a new action (",(0,i.jsx)(n.code,{children:"action == 2"}),") for building Supply Depots."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Implements the event-based penalty for becoming supply-blocked by tracking the previous step's supply state."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Provides the new, expanded 4-feature observation vector."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Updates the termination condition to a higher worker count (30)."]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ",(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"MacroEnv"})," (",(0,i.jsx)(n.code,{children:"gymnasium.Env"})," subclass):"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Defines the expanded ",(0,i.jsx)(n.code,{children:"action_space"})," as ",(0,i.jsx)(n.code,{children:"Discrete(3)"}),"."]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",checked:!0,disabled:!0})," ","Defines the expanded ",(0,i.jsx)(n.code,{children:"observation_space"})," as a ",(0,i.jsx)(n.code,{children:"Box"})," with a shape of ",(0,i.jsx)(n.code,{children:"(4,)"}),"."]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"macro_bot.py"})})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import numpy as np\r\nimport gymnasium as gym\r\nfrom gymnasium.spaces import Box, Discrete\r\nimport multiprocessing as mp\r\nfrom queue import Empty\r\n\r\nfrom sc2.bot_ai import BotAI\r\nfrom sc2.race import Terran\r\nfrom sc2.ids.unit_typeid import UnitTypeId\r\nfrom sc2_gym_env import SC2GymEnv, ObservationQueueItem\r\n\r\n# --- The BotAI Implementation (Runs in the Game Process) ---\r\n\r\nclass MacroBot(BotAI):\r\n    """\r\n    The BotAI actor for learning a simple macro trade-off:\r\n    when to build a worker vs. when to build a supply depot.\r\n    """\r\n    def __init__(self, action_queue: mp.Queue, obs_queue: mp.Queue[ObservationQueueItem]):\r\n        super().__init__()\r\n        self.action_queue = action_queue\r\n        self.obs_queue = obs_queue\r\n        self.race = Terran\r\n        # We need to track the last step\'s supply to penalize the frame we get blocked.\r\n        self.last_supply_left = 15\r\n\r\n    async def _handle_action(self, action: int) -> float:\r\n        """\r\n        Executes the agent\'s action and calculates the sparse reward.\r\n        Returns the reward for the action.\r\n        """\r\n        # Action 1: Build SCV\r\n        if action == 1 and self.can_afford(UnitTypeId.SCV) and self.townhalls.idle.exists:\r\n            self.train(UnitTypeId.SCV)\r\n            return 1.0\r\n\r\n        # Action 2: Build Supply Depot\r\n        elif action == 2 and self.can_afford(UnitTypeId.SUPPLYDEPOT) and not self.already_pending(UnitTypeId.SUPPLYDEPOT):\r\n            # We need to find a valid location to build the depot.\r\n            build_location = await self.find_placement(UnitTypeId.SUPPLYDEPOT, near=self.start_location, placement_step=5)\r\n            if build_location:\r\n                await self.build(UnitTypeId.SUPPLYDEPOT, build_location)\r\n                # Slightly higher reward to incentivize this crucial action.\r\n                return 2.0\r\n        \r\n        # Return 0 reward if the action was "Do Nothing" or was impossible.\r\n        return 0.0\r\n\r\n    async def on_step(self, iteration: int):\r\n        """\r\n        The main game loop, throttled to interact with the agent every 8 steps.\r\n        """\r\n        if iteration % 8 != 0:\r\n            return\r\n\r\n        try:\r\n            action = self.action_queue.get(timeout=1)\r\n\r\n            # Get the sparse reward from executing the action.\r\n            reward = await self._handle_action(action)\r\n\r\n            # Add an event-based penalty for getting supply blocked.\r\n            # This is a strong signal to teach the agent to avoid this state.\r\n            if self.supply_left == 0 and self.last_supply_left > 0:\r\n                reward -= 10.0\r\n            self.last_supply_left = self.supply_left\r\n\r\n            # Get the observation for the next state.\r\n            observation = np.array([\r\n                self.minerals / 1000.0,\r\n                self.workers.amount / 50.0,\r\n                self.supply_used / 200.0,\r\n                self.supply_cap / 200.0\r\n            ], dtype=np.float32)\r\n\r\n            # Define the termination condition.\r\n            terminated = self.workers.amount >= 30\r\n\r\n            # Send the data to the agent.\r\n            self.obs_queue.put((observation, reward, terminated, False, {}))\r\n\r\n            if terminated:\r\n                await self.client.leave()\r\n\r\n        except Empty:\r\n            print("Action queue was empty. Assuming training has ended.")\r\n            await self.client.leave()\r\n            return\r\n\r\n# --- The Gymnasium Environment (Runs in the Main Process) ---\r\n\r\nclass MacroEnv(SC2GymEnv):\r\n    """\r\n    The Gymnasium Wrapper for the MacroBot.\r\n    """\r\n    def __init__(self):\r\n        super().__init__(bot_class=MacroBot, map_name="AcropolisLE")\r\n        \r\n        # Action space: 0=Nothing, 1=Build SCV, 2=Build Supply Depot\r\n        self.action_space = Discrete(3)\r\n        \r\n        # Observation space: A 1D array with 4 normalized float values\r\n        self.observation_space = Box(\r\n            low=0.0,\r\n            high=1.0,\r\n            shape=(4,),\r\n            dtype=np.float32\r\n        )\n'})})]})}function p(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>a});var r=t(6540);const i={},o=r.createContext(i);function s(e){const n=r.useContext(o);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(o.Provider,{value:n},e.children)}}}]);