"use strict";(self.webpackChunkmy_framework_docs=self.webpackChunkmy_framework_docs||[]).push([[96],{3198:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks","title":"10.1 - The Concept - From Simple to Complex Tasks","description":"A significant challenge in reinforcement learning is that training an agent on a complex, final task from a random starting point can be extraordinarily inefficient or even impossible. The agent often fails to discover the rare, early-stage actions that lead to long-term success.","source":"@site/docs/Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/04-Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks.md","sourceDirName":"Part 5 - Reinforcement Learning/2 - Advanced RL and Next Steps/04-Chapter 10 - Improving Your Agent with Curriculum Learning","slug":"/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.1 - The Concept - From Simple to Complex Tasks","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"9.3 - Code - Implementing an Action Mask","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 9 - Improving Your Agent with Action Masking/9.3 - Code - Implementing an Action Mask"},"next":{"title":"10.2 - Example - A Curriculum for the Micro Bot","permalink":"/learn-python-sc2/docs/Part 5 - Reinforcement Learning/Advanced RL and Next Steps/Chapter 10 - Improving Your Agent with Curriculum Learning/10.2 - Example - A Curriculum for the Micro Bot"}}');var i=t(4848),a=t(8453);const s={},o=void 0,l={},c=[{value:"<strong>The Core Idea- Bootstrapping a Policy</strong>",id:"the-core-idea--bootstrapping-a-policy",level:4},{value:"<strong>Why This Approach is Effective</strong>",id:"why-this-approach-is-effective",level:4},{value:"<strong>Curriculum Design Principles</strong>",id:"curriculum-design-principles",level:4}];function d(e){const n={code:"code",h4:"h4",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.p,{children:"A significant challenge in reinforcement learning is that training an agent on a complex, final task from a random starting point can be extraordinarily inefficient or even impossible. The agent often fails to discover the rare, early-stage actions that lead to long-term success."}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Curriculum Learning"})," is an advanced training strategy that solves this problem by structuring the learning process as a sequence of tasks with increasing difficulty. This mirrors how humans learn, by mastering foundational skills before moving on to more complex applications."]}),"\n",(0,i.jsx)(n.h4,{id:"the-core-idea--bootstrapping-a-policy",children:(0,i.jsx)(n.strong,{children:"The Core Idea- Bootstrapping a Policy"})}),"\n",(0,i.jsxs)(n.p,{children:["Instead of a single, monolithic training run, we create a ",(0,i.jsx)(n.strong,{children:"curriculum"}),' of progressively harder environments. The agent\'s learned policy from an easier stage is used as the starting point, or "bootstrap," for training on the next, harder stage. This is a form of ',(0,i.jsx)(n.strong,{children:"Transfer Learning"}),"."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"The Curriculum Learning Workflow:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"+--------------------------------+\r\n|  Stage 1: Foundational Task    |\r\n|  (e.g., 1 Marine vs 1 Zergling)|\r\n+--------------------------------+\r\n               |\r\n               v\r\n+--------------------------------+\r\n| Train Agent A until proficient.|\r\n| (Save weights to `model_A.zip`)|\r\n+--------------------------------+\r\n               |\r\n               | Load weights from `model_A.zip`\r\n               v\r\n+--------------------------------+\r\n|  Stage 2: Intermediate Task    |\r\n|  (e.g., 2 Marines vs 2 Zerglings)|\r\n+--------------------------------+\r\n               |\r\n               v\r\n+--------------------------------+\r\n| Continue training Agent B.     |\r\n| (Save weights to `model_B.zip`)|\r\n+--------------------------------+\r\n               |\r\n               | Load weights from `model_B.zip`\r\n               v\r\n+--------------------------------+\r\n|  Stage 3: Target Task          |\r\n|  (e.g., 5v5 mixed combat)      |\r\n+--------------------------------+\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"why-this-approach-is-effective",children:(0,i.jsx)(n.strong,{children:"Why This Approach is Effective"})}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Solves Sparse Reward and Credit Assignment:"})," In early, simple stages, positive rewards are dense and easily attributable to specific actions. The agent can quickly learn a basic, valuable policy (e.g., how to shoot and move)."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Guided, Meaningful Exploration:"})," The agent's exploration is focused. It builds upon a foundation of competence rather than exploring from a state of complete randomness, which is highly inefficient in a large state space."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Improved Training Stability and Speed:"})," By starting each new stage with a competent policy, the agent often converges on a high-performance solution for the final, complex task much faster and more reliably than an agent trained from scratch on that same task."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h4,{id:"curriculum-design-principles",children:(0,i.jsx)(n.strong,{children:"Curriculum Design Principles"})}),"\n",(0,i.jsx)(n.p,{children:"Designing an effective curriculum is an engineering task that requires careful planning."}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Principle"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Description"}),(0,i.jsx)(n.th,{style:{textAlign:"left"},children:"Example"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Atomic Skills"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Each stage should isolate and teach a single new skill or concept."}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Stage 1: Teach 1v1 kiting. Stage 2: Teach target selection (2v1). Stage 3: Teach group cohesion (2v2)."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Smooth Difficulty Ramp"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"The increase in difficulty between stages should be gradual. A jump that is too large can cause the agent to fail to adapt."}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:"Good: 1v1 -> 2v1 -> 2v2. Bad: 1v1 -> 10v10."})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Quantifiable Graduation"})}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:'There must be a clear, measurable metric to determine when an agent has "mastered" a stage and is ready to advance.'}),(0,i.jsx)(n.td,{style:{textAlign:"left"},children:'"The agent graduates from Stage 1 when it achieves a >95% win rate against the built-in AI on the 1v1 task."'})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{style:{textAlign:"left"},children:(0,i.jsx)(n.strong,{children:"Environment Parameterization"})}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["The ",(0,i.jsx)(n.code,{children:"gymnasium"})," environment should be designed to be easily configurable so that a single ",(0,i.jsx)(n.code,{children:"Env"})," class can represent all stages of the curriculum."]}),(0,i.jsxs)(n.td,{style:{textAlign:"left"},children:["The ",(0,i.jsx)(n.code,{children:"MicroEnv"})," ",(0,i.jsx)(n.code,{children:"__init__"})," could accept parameters like ",(0,i.jsx)(n.code,{children:"(num_marines, num_zerglings)"}),"."]})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:["In the next section, we will design a concrete example of a simple curriculum for our ",(0,i.jsx)(n.code,{children:"MicroBot"}),", demonstrating how to put these principles into practice."]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>s,x:()=>o});var r=t(6540);const i={},a=r.createContext(i);function s(e){const n=r.useContext(a);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);